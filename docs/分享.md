1.数据库索引三个字段创建的唯一索引，在什么情况下会索引失败。

​	如果条件中有or，即使其中有部分条件带索引也不会使用(这也是为什么尽量少用or的原因)，
​	对于多列索引，不是使用的第一部分，则不会使用索引
​	like查询是以%开头
​	存在索引列的数据类型隐形转换，则用不上索引
​	where 子句里对索引列上有数学运算，用不上索引
​	where 子句里对有索引列使用函数，用不上索引
​	

#### 2.数据库索引的类型

​	Hash 索引结构的特殊性，其检索效率非常高，索引的检索可以一次定位，不像B-Tree 索引需要从根节点到枝节点，最后才能访问到页节点这样多次的IO访问，所以 Hash 索引的查询效率要远高于 B-Tree 索引。
​	Hash 索引的效率要比 B-Tree 高很多，为什么大家不都用 Hash 索引而还要使用 B-Tree 索引呢？
​		（1）Hash 索引仅仅能满足"=","IN"和"<=>"查询，不能使用范围查询。
​		由于 Hash 索引比较的是进行 Hash 运算之后的 Hash 值，所以它只能用于等值的过滤，不能用于基于范围的过滤，因为经过相应的 Hash 算法处理之后的 Hash 值的大小关系，并不能保证和Hash运算前完全一样。
​		（2）Hash 索引无法被用来避免数据的排序操作。
​		由于 Hash 索引中存放的是经过 Hash 计算之后的 Hash 值，而且Hash值的大小关系并不一定和 Hash 运算前的键值完全一样，所以数据库无法利用索引的数据来避免任何排序运算；
​		（3）Hash 索引不能利用部分索引键查询。
​		对于组合索引，Hash 索引在计算 Hash 值的时候是组合索引键合并后再一起计算 Hash 值，而不是单独计算 Hash 值，所以通过组合索引的前面一个或几个索引键进行查询的时候，Hash 索引也无法被利用。
​		（4）Hash 索引在任何时候都不能避免表扫描。
​		前面已经知道，Hash 索引是将索引键通过 Hash 运算之后，将 Hash运算结果的 Hash 值和所对应的行指针信息存放于一个 Hash 表中，由于不同索引键存在相同 Hash 值，所以即使取满足某个 Hash 键值的数据的记录条数，也无法从 Hash 索引中直接完成查询，还是要通过访问表中的实际数据进行相应的比较，并得到相应的结果。
​		（5）Hash 索引遇到大量Hash值相等的情况后性能并不一定就会比B-Tree索引高。
​		对于选择性比较低的索引键，如果创建 Hash 索引，那么将会存在大量记录指针信息存于同一个 Hash 值相关联。这样要定位某一条记录时就会非常麻烦，会浪费多次表数据的访问，而造成整体性能低下

缓存机制
MySQL缓存主要包括关键字缓存（key cache）和查询缓存（query cache），这主要讲解MySQL的查询缓存（query cache）机制。
查询缓存：在mysql的性能优化方面经常涉及到缓冲区（buffer）和缓存（cache）
	缓存之所以有效，主要是因为程序运行时对内存或者外存的访问呈现局部性特征，局部性特征为空间局部性和时间局部性两方面。时间局部性是指刚刚访问过的数据近期可能再次被访问，空间局部性是指，某个位置被访问后，其相邻的位置的数据很可能被访问到。而MySQL的缓存机制就是把刚刚访问的数据（时间局部性）以及未来即将访问到的数据（空间局部性）保存到缓存中，甚至是高速缓存中。从而提高I/O效率。
	按照缓存读写功能的不同，MySQL将缓存分为Buffer缓存和Cache缓存。
	Buffer缓存。由于硬盘的写入速度过慢，或者频繁的I/O，对于硬盘来说是极大的效率浪费。那么可以等到缓存中储存一定量的数据之后，一次性的写入到硬盘中。Buffer 缓存主要用于写数据，提升I/O性能。
	Cache 缓存。 Cache 缓存一般是一些访问频繁但是变更较少的数据，如果Cache缓存已经存储满，则启用LRU算法，进行数据淘汰。淘汰掉最远未使用的数据，从而开辟新的存储空间。不过对于特大型的网站，依靠这种策略很难缓解高频率的读请求，一般会把访问非常频繁的数据静态化，直接由nginx返还给用户。程序和数据库I/O设备交互的越少，则效率越高。

3.数据库乐观锁和悲观锁
	乐观锁不是数据库自带的，需要我们自己去实现。乐观锁是指操作数据库时(更新操作)，想法很乐观，认为这次的操作不会导致冲突，在操作数据时，并不进行任何其他的特殊处理（也就是不加锁），而在进行更新后，再去判断是否有冲突了。
	通常实现是这样的：在表中的数据进行操作时(更新)，先给数据表加一个版本(version)字段，每操作一次，将那条记录的版本号加1。也就是先查询出那条记录，获取出version字段,如果要对那条记录进行操作(更新),则先判断此刻version的值是否与刚刚查询出来时的version的值相等，如果相等，则说明这段期间，没有其他程序对其进行操作，则可以执行更新，将version字段的值加1；如果更新时发现此刻的version值与刚刚获取出来的version的值不相等，则说明这段期间已经有其他程序对其进行操作了，则不进行更新操作。
	与乐观锁相对应的就是悲观锁了。悲观锁就是在操作数据时，认为此操作会出现数据冲突，所以在进行每次操作时都要通过获取锁才能进行对相同数据的操作，这点跟java中的synchronized很相似，所以悲观锁需要耗费较多的时间。另外与乐观锁相对应的，悲观锁是由数据库自己实现了的，要用的时候，我们直接调用数据库的相关语句就可以了。
	说到这里，由悲观锁涉及到的另外两个锁概念就出来了，它们就是共享锁与排它锁。共享锁和排它锁是悲观锁的不同的实现，它俩都属于悲观锁的范畴。
	

共享锁又称为读锁，简称S锁，顾名思义，共享锁就是多个事务对于同一数据可以共享一把锁，都能访问到数据，但是只能读不能修改。
排他锁又称为写锁，简称X锁，顾名思义，排他锁就是不能与其他所并存，如一个事务获取了一个数据行的排他锁，其他事务就不能再获取该行的其他锁，包括共享锁和排他锁，但是获取排他锁的事务是可以对数据就行读取和修改。
对于共享锁大家可能很好理解，就是多个事务只能读数据不能改数据，对于排他锁大家的理解可能就有些差别，我当初就犯了一个错误，以为排他锁锁住一行数据后，其他事务就不能读取和修改该行数据，其实不是这样的。排他锁指的是一个事务在一行数据加上排他锁后，其他事务不能再在其上加其他的锁。mysql InnoDB引擎默认的修改数据语句，update,delete,insert都会自动给涉及到的数据加上排他锁，select语句默认不会加任何锁类型，如果加排他锁可以使用select ...for update语句，加共享锁可以使用select ... lock in share mode语句。所以加过排他锁的数据行在其他事务种是不能修改数据的，也不能通过for update和lock in share mode锁的方式查询数据，但可以直接通过select ...from...查询数据，因为普通查询没有任何锁机制。

4.redis 为什么快，为什么是单线程
	字符串（Strings），散列（Hash），列表（List），集合（Set），有序集合（Sorted Set或者是ZSet）
	1、完全基于内存，绝大部分请求是纯粹的内存操作，非常快速。数据存在内存中，类似于HashMap，HashMap的优势就是查找和操作的时间复杂度都是O(1)；
	2、数据结构简单，对数据操作也简单，Redis中的数据结构是专门进行设计的；
	3、采用单线程，避免了不必要的上下文切换和竞争条件，也不存在多进程或者多线程导致的切换而消耗 CPU，不用去考虑各种锁的问题，不存在加锁释放锁操作，没有因为可能出现死锁而导致的性能消耗；
	4、使用多路I/O复用模型，非阻塞IO；
	5、使用底层模型不同，它们之间底层实现方式以及与客户端之间通信的应用协议不一样，Redis直接自己构建了VM 机制 ，因为一般的系统调用系统函数的话，会浪费一定的时间去移动和请求；
	以上几点都比较好理解，下边我们针对多路 I/O 复用模型进行简单的探讨：
	（1）多路 I/O 复用模型
	多路I/O复用模型是利用 select、poll、epoll 可以同时监察多个流的 I/O 事件的能力，在空闲的时候，会把当前线程阻塞掉，当有一个或多个流有 I/O 事件时，就从阻塞态中唤醒，于是程序就会轮询一遍所有的流（epoll 是只轮询那些真正发出了事件的流），并且只依次顺序的处理就绪的流，这种做法就避免了大量的无用操作。
	这里“多路”指的是多个网络连接，“复用”指的是复用同一个线程。采用多路 I/O 复用技术可以让单个线程高效的处理多个连接请求（尽量减少网络 IO 的时间消耗），且 Redis 在内存中操作数据的速度非常快，也就是说内存内的操作不会成为影响Redis性能的瓶颈，主要由以上几点造就了 Redis 具有很高的吞吐量。
	redis持久化的几种方式
		RDB持久化是指在指定的时间间隔内将内存中的数据集快照写入磁盘，实际操作过程是fork一个子进程，先将数据集写入临时文件，写入成功后，再替换之前的文件，用二进制压缩存储。
		AOF持久化以日志的形式记录服务器所处理的每一个写、删除操作，查询操作不会记录，以文本的方式记录，可以打开文件看到详细的操作记录。
			RDB存在哪些优势呢？
			1). 一旦采用该方式，那么你的整个Redis数据库将只包含一个文件，这对于文件备份而言是非常完美的。比如，你可能打算每个小时归档一次最近24小时的数 据，同时还要每天归档一次最近30天的数据。通过这样的备份策略，一旦系统出现灾难性故障，我们可以非常容易的进行恢复。
			2). 对于灾难恢复而言，RDB是非常不错的选择。因为我们可以非常轻松的将一个单独的文件压缩后再转移到其它存储介质上。
			3). 性能最大化。对于Redis的服务进程而言，在开始持久化时，它唯一需要做的只是fork出子进程，之后再由子进程完成这些持久化的工作，这样就可以极大的避免服务进程执行IO操作了。
			4). 相比于AOF机制，如果数据集很大，RDB的启动效率会更高。
			RDB又存在哪些劣势呢？
			1). 如果你想保证数据的高可用性，即最大限度的避免数据丢失，那么RDB将不是一个很好的选择。因为系统一旦在定时持久化之前出现宕机现象，此前没有来得及写入磁盘的数据都将丢失。
			2). 由于RDB是通过fork子进程来协助完成数据持久化工作的，因此，如果当数据集较大时，可能会导致整个服务器停止服务几百毫秒，甚至是1秒钟。
			AOF的优势有哪些呢？
			1). 该机制可以带来更高的数据安全性，即数据持久性。Redis中提供了3中同步策略，即每秒同步、每修改同步和不同步。事实上，每秒同步也是异步完成的，其 效率也是非常高的，所差的是一旦系统出现宕机现象，那么这一秒钟之内修改的数据将会丢失。而每修改同步，我们可以将其视为同步持久化，即每次发生的数据变 化都会被立即记录到磁盘中。可以预见，这种方式在效率上是最低的。至于无同步，无需多言，我想大家都能正确的理解它。
			2). 由于该机制对日志文件的写入操作采用的是append模式，因此在写入过程中即使出现宕机现象，也不会破坏日志文件中已经存在的内容。然而如果我们本次操 作只是写入了一半数据就出现了系统崩溃问题，不用担心，在Redis下一次启动之前，我们可以通过redis-check-aof工具来帮助我们解决数据 一致性的问题。
			3). 如果日志过大，Redis可以自动启用rewrite机制。即Redis以append模式不断的将修改数据写入到老的磁盘文件中，同时Redis还会创 建一个新的文件用于记录此期间有哪些修改命令被执行。因此在进行rewrite切换时可以更好的保证数据安全性。
			4). AOF包含一个格式清晰、易于理解的日志文件用于记录所有的修改操作。事实上，我们也可以通过该文件完成数据的重建。
			AOF的劣势有哪些呢？
			1). 对于相同数量的数据集而言，AOF文件通常要大于RDB文件。RDB 在恢复大数据集时的速度比 AOF 的恢复速度要快。
			2). 根据同步策略的不同，AOF在运行效率上往往会慢于RDB。总之，每秒同步策略的效率是比较高的，同步禁用策略的效率和RDB一样高效。
			二者选择的标准，就是看系统是愿意牺牲一些性能，换取更高的缓存一致性（aof），还是愿意写操作频繁的时候，不启用备份来换取更高的性能，待手动运行save的时候，再做备份（rdb）。rdb这个就更有些 eventually consistent的意思了。

#### 5.java单例

##### 单例模式的八种写法

###### 1、饿汉式（静态常量）[可用]
	

```
public class Singleton {
		private final static Singleton INSTANCE = new Singleton();
		private Singleton(){}
		public static Singleton getInstance(){
			return INSTANCE;
		}
	}
```

​	优点：这种写法比较简单，就是在类装载的时候就完成实例化。避免了线程同步问题。
​	缺点：在类装载的时候就完成实例化，没有达到Lazy Loading的效果。如果从始至终从未使用过这个实例，则会造成内存的浪费。

###### 2、饿汉式（静态代码块）[可用]

```
public class Singleton {
	private static Singleton instance;
	static {
		instance = new Singleton();
	}
	private Singleton() {}

	public Singleton getInstance() {
		return instance;
	}
}
```


这种方式和上面的方式其实类似，只不过将类实例化的过程放在了静态代码块中，也是在类装载的时候，就执行静态代码块中的代码，初始化类的实例。优缺点和上面是一样的。

###### 3、懒汉式(线程不安全)[不可用]

```
public class Singleton {
	private static Singleton singleton;
	private Singleton() {}
	public static Singleton getInstance() {
		if (singleton == null) {
			singleton = new Singleton();
		}
		return singleton;
	}
}
```

这种写法起到了Lazy Loading的效果，但是只能在单线程下使用。如果在多线程下，一个线程进入了if (singleton == null)判断语句块，还未来得及往下执行，另一个线程也通过了这个判断语句，这时便会产生多个实例。所以在多线程环境下不可使用这种方式。

###### 4、懒汉式(线程安全，同步方法)[不推荐用]

```
public class Singleton {
	private static Singleton singleton;
	private Singleton() {}
	public static synchronized Singleton getInstance() {
		if (singleton == null) {
			singleton = new Singleton();
		}
		return singleton;
	}
}
```

解决上面第三种实现方式的线程不安全问题，做个线程同步就可以了，于是就对getInstance()方法进行了线程同步。
缺点：效率太低了，每个线程在想获得类的实例时候，执行getInstance()方法都要进行同步。而其实这个方法只执行一次实例化代码就够了，后面的想获得该类实例，直接return就行了。方法进行同步效率太低要改进。

###### 5、懒汉式(线程安全，同步代码块)[不可用]

```
public class Singleton {
	private static Singleton singleton;
	private Singleton() {}
	public static Singleton getInstance() {
		if (singleton == null) {
			synchronized (Singleton.class) {
				singleton = new Singleton();
			}
		}
		return singleton;
	}
}
```

由于第四种实现方式同步效率太低，所以摒弃同步方法，改为同步产生实例化的的代码块。但是这种同步并不能起到线程同步的作用。跟第3种实现方式遇到的情形一致，假如一个线程进入了if (singleton == null)判断语句块，还未来得及往下执行，另一个线程也通过了这个判断语句，这时便会产生多个实例。

###### 6、双重检查[推荐用]

```
public class Singleton {
	private static volatile Singleton singleton = null;
	private Singleton() {}
	public static Singleton getInstance() {
		if (singleton == null) {
			synchronized (Singleton.class) {
				if (singleton == null) {
					singleton = new Singleton();
				}
			}
		}
		return singleton;
	}
}
```

Double-Check概念对于多线程开发者来说不会陌生，如代码中所示，我们进行了两次if (singleton == null)检查，这样就可以保证线程安全了。这样，实例化代码只用执行一次，后面再次访问时，判断if (singleton == null)，直接return实例化对象。
优点：线程安全；延迟加载；效率较高。

###### 7、静态内部类[推荐用]

```
public class Singleton {
	private Singleton() {}
	private static class SingletonInstance {
		private static final Singleton INSTANCE = new Singleton();
	}
	public static Singleton getInstance() {
		return SingletonInstance.INSTANCE;
	}
}
```

这种方式跟饿汉式方式采用的机制类似，但又有不同。两者都是采用了类装载的机制来保证初始化实例时只有一个线程。不同的地方在饿汉式方式是只要Singleton类被装载就会实例化，没有Lazy-Loading的作用，而静态内部类方式在Singleton类被装载时并不会立即实例化，而是在需要实例化时，调用getInstance方法，才会装载SingletonInstance类，从而完成Singleton的实例化。
类的静态属性只会在第一次加载类的时候初始化，所以在这里，JVM帮助我们保证了线程的安全性，在类进行初始化时，别的线程是无法进入的。
优点：避免了线程不安全，延迟加载，效率高。

###### 8、枚举[推荐用]

```
public enum Singleton {
	INSTANCE;
	public void whateverMethod() {

	}
}
```


借助JDK1.5中添加的枚举来实现单例模式。不仅能避免多线程同步问题，而且还能防止反序列化重新创建新的对象。可能是因为枚举在JDK1.5中才添加，所以在实际项目开发中，很少见人这么写过。
优点
系统内存中该类只存在一个对象，节省了系统资源，对于一些需要频繁创建销毁的对象，使用单例模式可以提高系统性能。
缺点
当想实例化一个单例类的时候，必须要记住使用相应的获取对象的方法，而不是使用new，可能会给其他开发人员造成困扰，特别是看不到源码的时候。
适用场合
需要频繁的进行创建和销毁的对象；
创建对象时耗时过多或耗费资源过多，但又经常用到的对象；
工具类对象；
频繁访问数据库或文件的对象。

#### 6.java锁(synchronized jdk1.5前后有什么不同)

​	在JDK1.5之前同步使用的是Synchronized的方法，而线程的通信使用的是wait、Notify、NotifyAll等方法, 那么1.5之后就可以使用新的类库 Lock、Condition来完成。使用新的类库有一个显著的优势在于: 1.5之前线程的通信的等待和提醒绑定的是对象,例如Object等。而现在可以实现不同线程绑定不同的Condition用于提醒和等待这样，工作效率会大幅增加。
​		synchronized锁住的是对象只有对象是同一个才会生效
​		static synchronized方法，static方法可以直接类名加方法名调用，方法中无法使用this，所以它锁的不是this，而是类的Class对象，所以，static synchronized方法也相当于全局锁，相当于锁住了代码段。
​		不是static修饰的方法或者代码块。锁住的是this。synchronized代码块后随意加对象，synchronized后的括号中锁同一个固定对象，这样就行了。这样是没问题，但是，比较多的做法是让synchronized锁这个类对应的Class对象。
​	

总的来说，Lock和synchronized有以下几点不同：

​	(1) Lock是一个接口，是JDK层面的实现；而synchronized是Java中的关键字，是Java的内置特性，是JVM层面的实现；
​	(2) synchronized 在发生异常时，会自动释放线程占有的锁，因此不会导致死锁现象发生；而Lock在发生异常时，如果没有主动通过unLock()去释放锁，则很可能造成死锁现象，因此使用Lock时需要在finally块中释放锁；
​	(3) Lock 可以让等待锁的线程响应中断，而使用synchronized时，等待的线程会一直等待下去，不能够响应中断；
​	(4) 通过Lock可以知道有没有成功获取锁，而synchronized却无法办到；
​	(5) Lock可以提高多个线程进行读操作的效率。
​	在性能上来说，如果竞争资源不激烈，两者的性能是差不多的。而当竞争资源非常激烈时（即有大量线程同时竞争），此时Lock的性能要远远优于synchronized。所以说，在具体使用时要根据适当情况选择。
​	
锁的相关概念介绍
​	1、可重入锁

​	如果锁具备可重入性，则称作为 可重入锁 。像 synchronized和ReentrantLock都是可重入锁，可重入性在我看来实际上表明了 锁的分配机制：基于线程的分配，而不是基于方法调用的分配。
​	2、可中断锁

​	顾名思义，可中断锁就是可以响应中断的锁。在Java中，synchronized就不是可中断锁，而Lock是可中断锁。
​	如果某一线程A正在执行锁中的代码，另一线程B正在等待获取该锁，可能由于等待时间过长，线程B不想等待了，想先处理其他事情，我们可以让它中断自己或者在别的线程中中断它，这种就是可中断锁。
​	3、公平锁

​	公平锁即 尽量 以请求锁的顺序来获取锁。比如，同是有多个线程在等待一个锁，当这个锁被释放时，等待时间最久的线程（最先请求的线程）会获得该所，这种就是公平锁。而非公平锁则无法保证锁的获取是按照请求锁的顺序进行的，这样就可能导致某个或者一些线程永远获取不到锁。

​	在Java中，synchronized就是非公平锁，它无法保证等待的线程获取锁的顺序。而对于ReentrantLock 和 ReentrantReadWriteLock，它默认情况下是非公平锁，但是可以设置为公平锁。
​	
​	4.读写锁
​	读写锁将对临界资源的访问分成了两个锁，一个读锁和一个写锁。正因为有了读写锁，才使得多个线程之间的读操作不会发生冲突。ReadWriteLock就是读写锁，它是一个接口，ReentrantReadWriteLock实现了这个接口。可以通过readLock()获取读锁，通过writeLock()获取写锁。
​	
volatile
　　Volatile修饰的成员变量在每次被线程访问时，都强迫从主内存中重读该成员变量的值。而且，当成员变量发生变化时，强迫线程将变化值回写到主内存。这样在任何时刻，两个不同的线程总是看到某个成员变量的同一个值。
　　Java语言规范中指出：为了获得最佳速度，允许线程保存共享成员变量的私有拷贝，而且只当线程进入或者离开同步代码块时才与共享成员变量的原始值对比。
　　这样当多个线程同时与某个对象交互时，就必须要注意到要让线程及时的得到共享成员变量的变化。
　　而volatile关键字就是提示VM：对于这个成员变量不能保存它的私有拷贝，而应直接与共享成员变量交互。
　　使用建议：在两个或者更多的线程访问的成员变量上使用volatile。当要访问的变量已在synchronized代码块中，或者为常量时，不必使用。
　　由于使用volatile屏蔽掉了VM中必要的代码优化，所以在效率上比较低，因此一定在必要时才使用此关键字。

7.分布式事务所锁实现
	基于数据库、redis.
	setnx(lockkey, 当前时间+过期超时时间)，如果返回 1，则获取锁成功；如果返回 0 则没有获取到锁，转向 2。
	get(lockkey) 获取值 oldExpireTime ，并将这个 value 值与当前的系统时间进行比较，如果小于当前系统时间，则认为这个锁已经超时，可以允许别的请求重新获取，转向 3。
	计算 newExpireTime = 当前时间+过期超时时间，然后 getset(lockkey, newExpireTime) 会返回当前 lockkey 的值currentExpireTime。
	判断 currentExpireTime 与 oldExpireTime 是否相等，如果相等，说明当前 getset 设置成功，获取到了锁。如果不相等，说明这个锁又被别的请求获取走了，那么当前请求可以直接返回失败，或者继续重试。
	在获取到锁之后，当前线程可以开始自己的业务处理，当处理完毕后，比较自己的处理时间和对于锁设置的超时时间，如果小于锁设置的超时时间，则直接执行 delete 释放锁；如果大于锁设置的超时时间，则不需要再锁进行处理。
	http://www.cnblogs.com/seesun2012/p/9214653.html
8.二分查找的实现和时间复杂度
	

```
	/**
	 * 不使用递归的二分查找
	 *title:commonBinarySearch
	 *@param arr
	 *@param key
	 *@return 关键字位置
	 */
	public static int commonBinarySearch(int[] arr,int key){
		int low = 0;
		int high = arr.length - 1;
		int middle = 0;			//定义middle
		

	if(key < arr[low] || key > arr[high] || low > high){
		return -1;				
	}
	
	while(low <= high){
		middle = (low + high) / 2;
		if(arr[middle] > key){
			//比关键字大则关键字在左区域
			high = middle - 1;
		}else if(arr[middle] < key){
			//比关键字小则关键字在右区域
			low = middle + 1;
		}else{
			return middle;
		}
	}
	
	return -1;		//最后仍然没有找到，则返回-1
}
```

比如：总共有n个元素，每次查找的区间大小就是n，n/2，n/4，…，n/2^k（接下来操作元素的剩余个数），其中k就是循环的次数。 
由于n/2^k取整后>=1，即令n/2^k=1， 
可得k=log2n,（是以2为底，n的对数），所以时间复杂度可以表示O()=O(logn)

9.http缓存
	强制缓存
	从上文我们得知，强制缓存，在缓存数据未失效的情况下，可以直接使用缓存数据，那么浏览器是如何判断缓存数据是否失效呢？
	我们知道，在没有缓存数据的时候，浏览器向服务器请求数据时，服务器会将数据和缓存规则一并返回，缓存规则信息包含在响应header中。
	对于强制缓存来说，响应header中会有两个字段来标明失效规则（Expires/Cache-Control）
	Expires的值为服务端返回的到期时间，即下一次请求时，请求时间小于服务端返回的到期时间，直接使用缓存数据。
	不过Expires 是HTTP 1.0的东西，现在默认浏览器均默认使用HTTP 1.1，所以它的作用基本忽略。
	另一个问题是，到期时间是由服务端生成的，但是客户端时间可能跟服务端时间有误差，这就会导致缓存命中的误差。
	所以HTTP 1.1 的版本，使用Cache-Control替代。
		对比缓存，顾名思义，需要进行比较判断是否可以使用缓存。
	浏览器第一次请求数据时，服务器会将缓存标识与数据一起返回给客户端，客户端将二者备份至缓存数据库中。
	再次请求数据时，客户端将备份的缓存标识发送给服务器，服务器根据缓存标识进行判断，判断成功后，返回304状态码，通知客户端比较成功，可以使用缓存数据。
	Last-Modified  /  If-Modified-Since
	Etag  /  If-None-Match（优先级高于Last-Modified  /  If-Modified-Since）
10.dubbo异步调用
	异步通讯对于服务端响应时间较长的方法是必须的，能够有效地利用客户端的资源，在dubbo中，消费端dubbp:method通过 async="true"标识。
	具体有三种方式：
	1、NIO future主动获取结果，返回结果放在RpcContext中
		 需要注意的是，由于RpcContext是单例模式，所以每次调用完后，需要保存一个Future实例
		  fooService.findFoo(fooId);
		  Future<Foo> fooFuture = RpcContext.getContext().getFuture();
		  barService.findBar(barId);
		  Future<Bar> barFuture = RpcContext.getContext().getFuture();
		  barService.findBar(barId);
		  Bar bar = barFuture.get();
	2、通过回调（Callback）参数
		  Callback并不是dubbo内部类或接口，而是由应用自定义的、实现了Serializable的接口；
		  分两步：1）服务提供者需在方法中声明Callback参数，其后在Service实现中显示地调用Callback的方法；
		  <dubbo:service ..>
			 <dubbo:method name="method1">
				<dubbo:argument index="1" callback="true" />  #标识第二个参数是callback类型
			 /dubbo:method
		  /dubbo:service
		  2）Callback接口的实现类在消费端，当方法发生调用时，消费端会自动export一个Callback服务，在Rpc调用完成后，不能立即结束线程。
		  <dubbo:reference ...>
			 <dubbo:method name="method1" async="true">
		  /dubbo:reference
	3、事件通知（推荐）
		 这种方式更简单，对服务提供方来讲是透明的，包括配置和代码上，均无需做任何改动。
		 消费端定义一个“通知者”的Spring Bean，指定方法的onreturn和onthrow事件action就可以。
		  

```
<bean id="notify" class="com.alibaba.dubbo.callback.implicit.NofifyImpl" />
		  <dubbo:reference >
			<dubbo:method name="method1" async="true" onreturn="notify.onreturn" onthrow="notify.onthrow" />
		  /dubbo:reference
```

11.disconf的原理
	利用zookeeper的发布/订阅模式实现配置动态变更
		ZooKeeper的Watcher事件机制可以说分布式场景下的观察者模式的实现。基于这个watcher事件机制，配合注册到特定的ZNode节点，可以实现java应用的配置运行时的变更。在学习zookeeper之前，听同事说配置可以在运行时动态变更，觉得不可思议。研习了zookeeper之后，实现这个功能是很easy的。 
	  发布/订阅系统设计起来无非两种模式，推和拉。 
	1. 推模式，服务端负责把变更的数据推给订阅的客户端。Web即时通信里的Comet技术便可以实现这种功能。 
	2. 拉模式，也就是客户端定时轮询服务端。拉模式不仅有延迟，给服务端带来很大压力，而且十分低效。
	1. 推模式，服务端负责把变更的数据推给订阅的客户端。Web即时通信里的Comet技术便可以实现这种功能。 
	2. 拉模式，也就是客户端定时轮询服务端。拉模式不仅有延迟，给服务端带来很大压力，而且十分低效。

  zookeeper采用的是推拉结合的模式 
1. 客户端订阅znode节点 

2. 被订阅的节点发生变化后，zookeeper服务端向客户端发生数据变更的watcher事件通知 

3. 客户端接收到watcher通知后，主动从服务端拉取变更的数据
  阿里的配置变更中间件diamond，同样是基于推拉结合的模式来实现数据的动态变更。初学zookeeper，写了一个数据库配置动态变更的demo。zookeeper自带的Watcher注册后，数据变更一次便会自动取消注册。这个设计实在反人类，大多数的开发者的需求肯定是注册一次，服务终生。所以转向开源的ZkClient客户端。 
    首先本地需要启动一个zookeeper的服务端，并且有一个“/db”节点，我用这个节点存储数据库配置信息。 
    客户端工程需要引入zookeeper和zkclient的依赖。

  ```
  <dependency>
      <groupId>org.apache.zookeeper</groupId>
      <artifactId>zookeeper</artifactId>
      <version>3.4.9</version>
  </dependency>
  <dependency>
      <groupId>com.github.adyliu</groupId>
      <artifactId>zkclient</artifactId>
      <version>2.1.1</version>
  </dependency>
  ```

   

  ```
  1.  客户端demo
     import com.github.zkclient.IZkDataListener;
     import com.github.zkclient.ZkClient;
     import java.io.IOException;
  /**
  project  : zk
  - package  : PACKAGE_NAME
  - author   : lvsheng
  - date     : 2016/10/5 下午11:17
    */
    public class ZkClientTest {
  public static void main(String[] args) {
      ZkClient zkClient = new ZkClient("127.0.0.1:2181");
      zkClient.subscribeDataChanges("/db", new IZkDataListener() {
          public void handleDataChange(String dataPath, byte[] data) throws Exception {
          	System.out.println(new String(data));
          }
  
          public void handleDataDeleted(String dataPath) throws Exception {
          	System.out.println(dataPath);
          }
      });
  
      try {
      	System.in.read();
      } catch (IOException e) {
      	e.printStackTrace();
      }
      }
    }
  ```


    现在我们来测试这个功能，在控制台依次输入如下命令：
  [zk: localhost:2181(CONNECTED) 48] set /db chat.jdbc.driver=com.mysql.jdbc.Driver|chat.jdbc.url=jdbc:mysql://192.168.146.120:3306/chat_test?useUnicode=true&amp;characterEncoding=GBK|chat.jdbc.maxActive=5
  [zk: localhost:2181(CONNECTED) 49] set /db chat.jdbc.driver=com.mysql.jdbc.Driver|chat.jdbc.url=jdbc:mysql://127.0.0.1:3306/chat_test?useUnicode=true&amp;characterEncoding=GBK|chat.jdbc.maxActive=5          
    观察Java代码的输出
  chat.jdbc.driver=com.mysql.jdbc.Driver|chat.jdbc.url=jdbc:mysql://192.168.146.120:3306/chat_test?useUnicode=true&amp;characterEncoding=GBK|chat.jdbc.maxActive=5
  chat.jdbc.driver=com.mysql.jdbc.Driver|chat.jdbc.url=jdbc:mysql://127.0.0.1:3306/chat_test?useUnicode=true&amp;characterEncoding=GBK|chat.jdbc.maxActive=5
    数据库配置的ip被成功的动态修改。

12.自己实现rpc
13.map几种实现
	1、基本介绍
		HashMap、TreeMap、HashTable、LinkedHashMap 共同实现了接口java.util.Map， 都是键值对形式，且map的key不允许重复
	2、具体实现
		a、HashMap
			是一个最常用的Map实现方式,它根据键的HashCode 值存储数据,根据键可以直接获取它的值，具有很快的访问速度，但是HashMap是无序、线程不安全的，且HashMap不同步，如果需要线程同步，则可以使用ConcurrentHashMap，也可以使用Collections.synchronizedMap(HashMap map)方法让HashMap具有同步的能力。其实同步同步，就看有没有synchronized关键字。 HashMap的key 有且只能允许一个null。至于存取方式我就不说了
			注：线程不安全（多个线程访问同一个对象或实现进行更新操作时，造成数据混乱）
		b、HashTable
			HashTable继承自Dictionary类 ，它也是无序的，但是HashTable是线程安全的，同步的，即任一时刻只有一个线程能写HashTable， 但是这也让HashTable在读取的时候，速度比HashMap慢，但是写入速度是比HashMap快的
			之前我一直存在一个误区，以为HashMap的写入速度比HashTable快，但是测试表明，HashTable的写入快，读取慢
		c、LinkedHashMap
			LinkedHashMap是Map中常用的有序的两种实现之一， 它保存了记录的插入顺序，先插入的先遍历到，就是说你插入是什么顺序，你出来就是什么顺序。
			对于LinkedHashMap而言，它继承与HashMap、底层使用哈希表与双向链表来保存所有元素。其基本操作与父类HashMap相似，它通过重写父类相关的方法，来实现自己的链接列表特性。LinkedHashMap采用的hash算法和HashMap相同，但是它重新定义了数组中保存的元素Entry，该Entry除了保存当前对象的引用外，还保存了其上一个元素before和下一个元素after的引用，从而在哈希表的基础上又构成了双向链接列表
			注：LinkedHashMap在遍历的时候会比HashMap慢，不过有种情况例外，当HashMap容量很大，实际数据较少时，遍历起来可能会 比LinkedHashMap慢，因为LinkedHashMap的遍历速度只和实际数据有关，和容量无关，而HashMap的遍历速度和他的容量有关
		d、TreeMap
			TreeMap实现SortMap接口，能够把它保存的记录根据键排序,默认是按键值的升序排序，也可以指定排序的比较器，当用Iterator 遍历TreeMap时，得到的记录是排过序的。
			TreeMap的排序原理是：红黑树算法的实现 ，具体概念参考：点击打开链接。
			它的主要实现是Comparator架构，通过比较的方式，进行一个排序
	3、总结
		1、Map中，HashMap具有超高的访问速度，如果我们只是在Map 中插入、删除和定位元素，而无关线程安全或者同步问题，HashMap 是最好的选择。
		2、如果考虑线程安全或者写入速度的话，可以使用HashTable
		3、如果想按怎么存的顺序怎么取，比如队列形式，排排队。 那么使用LinkedHashMap吧，怎么用怎么爽
		4、如果需要让Map按照key进行升序或者降序排序，那就用TreeMap吧
14.list的几种实现
	List实现Collection接口，它的数据结构是有序可以重复的结合，该结合的体系有索引；它有三个实现类：ArrayList、LinkList、Vector三个实现类。

 2. 三个实现类的基本区别：
    	2.1 ArrayList：
    	底层数据结构使数组结构，查询速度快，增删改慢，
    	2.2 LinkList：
    	底层使用链表结构，增删速度快，查询稍慢；
    	2.3 Vector：
    	底层是数组结构，Vector是线程同步的，所以它也是线程安全的。而ArratList是线程异步的，不安全。如果不考虑安全因素，一般用Arralist效率比较高
    	可变长度数组不断new数组： 
    	（1） ArrayList当初始化容量超过10时，会new一个50%de ,把原来的东西放入这150%中； 
    	（2） Vector：当容量超过10时，会new一个100%的浪费内存；
    ```
     public ThreadPoolExecutor(int corePoolSize,
                               int maximumPoolSize,
                               long keepAliveTime,
                               TimeUnit unit,
                               BlockingQueue<Runnable> workQueue,
                               RejectedExecutionHandler handler) 
    ```

    ​		corePoolSize：线程池核心线程数量
    ​		maximumPoolSize:线程池最大线程数量
    ​		keepAliverTime：当活跃线程数大于核心线程数时，空闲的多余线程最大存活时间
    ​		unit：存活时间的单位
    ​		workQueue：存放任务的队列
    ​		handler：超出线程范围和队列容量的任务的处理程序
    ​	线程池的实现原理
    ​		提交一个任务到线程池中，线程池的处理流程如下：
    ​		1、判断线程池里的核心线程是否都在执行任务，如果不是（核心线程空闲或者还有核心线程没有被创建）则创建一个新的工作线程来执行任务。如果核心线程都在执行任务，则进入下个流程。
    ​		2、线程池判断工作队列是否已满，如果工作队列没有满，则将新提交的任务存储在这个工作队列里。如果工作队列满了，则进入下个流程。
    ​		3、判断线程池里的线程是否都处于工作状态，如果没有，则创建一个新的工作线程来执行任务。如果已经满了，则交给饱和策略来处理这个任务。
    3. ArrayList
    3.1 ArrayList 概述：
    	① ArrayList是List接口的可变数组的实现。实现了所有可选列表操作，并允许包括 null 在内的所有元素。除了实现 List 接口外，此类还提供一些方法来操作内部用来存储列表的数组的大小。
    	② 每个ArrayList实例都有一个容量，该容量是指用来存储列表元素的数组的大小。它总是至少等于列表的大小。随着向ArrayList中不断添加元素，其容量也自动增长。自动增长会带来数据向新数组的重新拷贝，因此，如果可预知数据量的多少，可在构造ArrayList时指定其容量。在添加大量元素前，应用程序也可以使用ensureCapacity操作来增加ArrayList实例的容量，这可以减少递增式再分配的数量。
    	③ 注意，此实现不是同步的。如果多个线程同时访问一个ArrayList实例，而其中至少一个线程从结构上修改了列表，那么它必须保持外部同步。
    	3.2 ArrayList的实现：
    	对于ArrayList而言，它实现List接口、底层使用数组保存所有元素。其操作基本上是对数组的操作。下面我们来分析ArrayList的源代码：底层使用数组实现
    	Java代码 private transient Object[] elementData;
    	构造方法： 
    	ArrayList提供了三种方式的构造器，可以构造一个默认初始容量为10的空列表、构造一个指定初始容量的空列表以及构造一个包含指定collection的元素的列表，这些元素按照该collection的迭代器返回它们的顺序排列的。
    4. LinkList
    	4.1 LinkList的概述：
    	LinkedList的本质是双向链表。
    	(01) LinkedList继承于AbstractSequentialList，并且实现了Dequeue接口。 
    	(02) LinkedList包含两个重要的成员：header 和 size。 
    	header是双向链表的表头，它是双向链表节点所对应的类Entry的实例。Entry中包含成员变量： previous, next, element。其中，previous是该节点的上一个节点，next是该节点的下一个节点，element是该节点所包含的值。 
    	size是双向链表中节点的个数。
    	4.2 LinkList的实现：
    	LinkedList实际上是通过双向链表去实现的。既然是双向链表，那么它的顺序访问会非常高效，而随机访问效率比较低。
    	既然LinkedList是通过双向链表的，但是它也实现了List接口{也就是说，它实现了get(int location)、remove(intlocation)等“根据索引值来获取、删除节点的函数”}。LinkedList是如何实现List的这些接口的，如何将“双向链表和索引值联系起来的”？
    	实际原理非常简单，它就是通过一个计数索引值来实现的。例如，当我们调用get(int location)时，首先会比较“location”和“双向链表长度的1/2”；若前者大，则从链表头开始往后查找，直到location位置；否则，从链表末尾开始先前查找，直到location位置。
    5. Vector
    	相对于ArrayList来说，Vector线程是安全的，也就是说是同步的 
    	创建了一个向量类的对象后，可以往其中随意地插入不同的类的对象，既不需顾及类型也不需预先选定向量的容量，并可方便地进行查找。对于预先不知或不愿预先定义数组大小，并需频繁进行查找、插入和删除工作的情况，可以考虑使用向量类。向量类提供了三种构造方法：
    	public vector()
    	public vector(intinitialcapacity,int capacityIncrement)
    	public vector(intinitialcapacity)
    	使用第一种方法，系统会自动对向量对象进行管理。若使用后两种方法，则系统将根据参数initialcapacity设定向量对象的容量（即向量对象可存储数据的大小），当真正存放的数据个数超过容量时，系统会扩充向量对象的储存容量。 
    	参数capacityIncrement给定了每次扩充的扩充值。当capacityIncrement为0时，则每次扩充一倍。利用这个功能可以优化存储。
    Set
    一个不包含重复元素的 collection。无序且唯一。
    HashSet
    LinkedHashSet
    TreeSet
    HashSet是使用哈希表（hash table）实现的，其中的元素是无序的。HashSet的add、remove、contains方法 的时间复杂度为常量O(1)。
    TreeSet使用树形结构（算法书中的红黑树red-black tree）实现的。TreeSet中的元素是可排序的，但add、remove和contains方法的时间复杂度为O(log(n))。TreeSet还提供了first()、last()、headSet()、tailSet()等方法来操作排序后的集合。
    LinkedHashSet介于HashSet和TreeSet之间。它基于一个由链表实现的哈希表，保留了元素插入顺序。LinkedHashSet中基本方法的时间复杂度为O(1)。
    15.为什么用多线程 线程池的原理
    线程池的优点
    	1、线程是稀缺资源，使用线程池可以减少创建和销毁线程的次数，每个工作线程都可以重复使用。
    	2、可以根据系统的承受能力，调整线程池中工作线程的数量，防止因为消耗过多内存导致服务器崩溃。
    线程池的创建
    	

	2. 三个实现类的基本区别：
		2.1 ArrayList：
		底层数据结构使数组结构，查询速度快，增删改慢，
		2.2 LinkList：
		底层使用链表结构，增删速度快，查询稍慢；
		2.3 Vector：
		底层是数组结构，Vector是线程同步的，所以它也是线程安全的。而ArratList是线程异步的，不安全。如果不考虑安全因素，一般用Arralist效率比较高
		可变长度数组不断new数组： 
		（1） ArrayList当初始化容量超过10时，会new一个50%de ,把原来的东西放入这150%中； 
		（2） Vector：当容量超过10时，会new一个100%的浪费内存；
	3. ArrayList
	3.1 ArrayList 概述：
		① ArrayList是List接口的可变数组的实现。实现了所有可选列表操作，并允许包括 null 在内的所有元素。除了实现 List 接口外，此类还提供一些方法来操作内部用来存储列表的数组的大小。
		② 每个ArrayList实例都有一个容量，该容量是指用来存储列表元素的数组的大小。它总是至少等于列表的大小。随着向ArrayList中不断添加元素，其容量也自动增长。自动增长会带来数据向新数组的重新拷贝，因此，如果可预知数据量的多少，可在构造ArrayList时指定其容量。在添加大量元素前，应用程序也可以使用ensureCapacity操作来增加ArrayList实例的容量，这可以减少递增式再分配的数量。
		③ 注意，此实现不是同步的。如果多个线程同时访问一个ArrayList实例，而其中至少一个线程从结构上修改了列表，那么它必须保持外部同步。
		3.2 ArrayList的实现：
		对于ArrayList而言，它实现List接口、底层使用数组保存所有元素。其操作基本上是对数组的操作。下面我们来分析ArrayList的源代码：底层使用数组实现
		Java代码 private transient Object[] elementData;
		构造方法： 
		ArrayList提供了三种方式的构造器，可以构造一个默认初始容量为10的空列表、构造一个指定初始容量的空列表以及构造一个包含指定collection的元素的列表，这些元素按照该collection的迭代器返回它们的顺序排列的。
	4. LinkList
		4.1 LinkList的概述：
		LinkedList的本质是双向链表。
		(01) LinkedList继承于AbstractSequentialList，并且实现了Dequeue接口。 
		(02) LinkedList包含两个重要的成员：header 和 size。 
		header是双向链表的表头，它是双向链表节点所对应的类Entry的实例。Entry中包含成员变量： previous, next, element。其中，previous是该节点的上一个节点，next是该节点的下一个节点，element是该节点所包含的值。 
		size是双向链表中节点的个数。
		4.2 LinkList的实现：
		LinkedList实际上是通过双向链表去实现的。既然是双向链表，那么它的顺序访问会非常高效，而随机访问效率比较低。
		既然LinkedList是通过双向链表的，但是它也实现了List接口{也就是说，它实现了get(int location)、remove(intlocation)等“根据索引值来获取、删除节点的函数”}。LinkedList是如何实现List的这些接口的，如何将“双向链表和索引值联系起来的”？
		实际原理非常简单，它就是通过一个计数索引值来实现的。例如，当我们调用get(int location)时，首先会比较“location”和“双向链表长度的1/2”；若前者大，则从链表头开始往后查找，直到location位置；否则，从链表末尾开始先前查找，直到location位置。
	5. Vector
		相对于ArrayList来说，Vector线程是安全的，也就是说是同步的 
		创建了一个向量类的对象后，可以往其中随意地插入不同的类的对象，既不需顾及类型也不需预先选定向量的容量，并可方便地进行查找。对于预先不知或不愿预先定义数组大小，并需频繁进行查找、插入和删除工作的情况，可以考虑使用向量类。向量类提供了三种构造方法：
		public vector()
		public vector(intinitialcapacity,int capacityIncrement)
		public vector(intinitialcapacity)
		使用第一种方法，系统会自动对向量对象进行管理。若使用后两种方法，则系统将根据参数initialcapacity设定向量对象的容量（即向量对象可存储数据的大小），当真正存放的数据个数超过容量时，系统会扩充向量对象的储存容量。 
		参数capacityIncrement给定了每次扩充的扩充值。当capacityIncrement为0时，则每次扩充一倍。利用这个功能可以优化存储。
Set
	一个不包含重复元素的 collection。无序且唯一。
	HashSet
	LinkedHashSet
	TreeSet
	HashSet是使用哈希表（hash table）实现的，其中的元素是无序的。HashSet的add、remove、contains方法 的时间复杂度为常量O(1)。
	TreeSet使用树形结构（算法书中的红黑树red-black tree）实现的。TreeSet中的元素是可排序的，但add、remove和contains方法的时间复杂度为O(log(n))。TreeSet还提供了first()、last()、headSet()、tailSet()等方法来操作排序后的集合。
	LinkedHashSet介于HashSet和TreeSet之间。它基于一个由链表实现的哈希表，保留了元素插入顺序。LinkedHashSet中基本方法的时间复杂度为O(1)。
15.为什么用多线程 线程池的原理
	线程池的优点
		1、线程是稀缺资源，使用线程池可以减少创建和销毁线程的次数，每个工作线程都可以重复使用。
		2、可以根据系统的承受能力，调整线程池中工作线程的数量，防止因为消耗过多内存导致服务器崩溃。
	线程池的创建
		
		corePoolSize：线程池核心线程数量
		maximumPoolSize:线程池最大线程数量
		keepAliverTime：当活跃线程数大于核心线程数时，空闲的多余线程最大存活时间
		unit：存活时间的单位
		workQueue：存放任务的队列
		handler：超出线程范围和队列容量的任务的处理程序
	线程池的实现原理
		提交一个任务到线程池中，线程池的处理流程如下：
		1、判断线程池里的核心线程是否都在执行任务，如果不是（核心线程空闲或者还有核心线程没有被创建）则创建一个新的工作线程来执行任务。如果核心线程都在执行任务，则进入下个流程。
		2、线程池判断工作队列是否已满，如果工作队列没有满，则将新提交的任务存储在这个工作队列里。如果工作队列满了，则进入下个流程。
		3、判断线程池里的线程是否都处于工作状态，如果没有，则创建一个新的工作线程来执行任务。如果已经满了，则交给饱和策略来处理这个任务。

RejectedExecutionHandler：饱和策略
	当队列和线程池都满了，说明线程池处于饱和状态，那么必须对新提交的任务采用一种特殊的策略来进行处理。这个策略默认配置是AbortPolicy，表示无法处理新的任务而抛出异常。JAVA提供了4中策略：
	1、AbortPolicy：直接抛出异常
	2、CallerRunsPolicy：只用调用所在的线程运行任务
	3、DiscardOldestPolicy：丢弃队列里最近的一个任务，并执行当前任务。
	4、DiscardPolicy：不处理，丢弃掉。

16.代理 动态、静态代理
	为某个对象提供一个代理，以控制对这个对象的访问。 代理类和委托类有共同的父类或父接口，这样在任何使用委托类对象的地方都可以用代理对象替代。代理类负责请求的预处理、过滤、将请求分派给委托类处理、以及委托类执行完请求后的后续处理。
	

静态代理
	由程序员创建或工具生成代理类的源码，再编译代理类。所谓静态也就是在程序运行前就已经存在代理类的字节码文件，代理类和委托类的关系在运行前就确定了。
	静态代理类优缺点
		优点：业务类只需要关注业务逻辑本身，保证了业务类的重用性。这是代理的共有优点。
		缺点：
		1）代理对象的一个接口只服务于一种类型的对象，如果要代理的方法很多，势必要为每一种方法都进行代理，静态代理在程序规模稍大时就无法胜任了。
		2）如果接口增加一个方法，除了所有实现类需要实现这个方法外，所有代理类也需要实现此方法。增加了代码维护的复杂度。

​		另外，如果要按照上述的方法使用代理模式，那么真实角色(委托类)必须是事先已经存在的，并将其作为代理对象的内部属性。但是实际使用时，一个真实角色必须对应一个代理角色，如果大量使用会导致类的急剧膨胀；此外，如果事先并不知道真实角色（委托类），该如何使用代理呢？这个问题可以通过Java的动态代理类来解决。
动态代理
​	动态代理类的源码是在程序运行期间由JVM根据反射等机制动态的生成，所以不存在代理类的字节码文件。代理类和委托类的关系是在程序运行时确定。
​	具体步骤是：
​		a. 实现InvocationHandler接口创建自己的调用处理器
​		b. 给Proxy类提供ClassLoader和代理接口类型数组创建动态代理类
​		c. 以调用处理器类型为参数，利用反射机制得到动态代理类的构造函数
​		d. 以调用处理器对象为参数，利用动态代理类的构造函数创建动态代理类对象
​	优点：
​		动态代理与静态代理相比较，最大的好处是接口中声明的所有方法都被转移到调用处理器一个集中的方法中处理（InvocationHandler.invoke）。这样，在接口方法数量比较多的时候，我们可以进行灵活处理，而不需要像静态代理那样每一个方法进行中转。在本示例中看不出来，因为invoke方法体内嵌入了具体的外围业务（记录任务处理前后时间并计算时间差），实际中可以类似Spring AOP那样配置外围业务。
​	美中不足：
​		诚然，Proxy 已经设计得非常优美，但是还是有一点点小小的遗憾之处，那就是它始终无法摆脱仅支持 interface 代理的桎梏，因为它的设计注定了这个遗憾。回想一下那些动态生成的代理类的继承关系图，它们已经注定有一个共同的父类叫 Proxy。Java 的继承机制注定了这些动态代理类们无法实现对 class 的动态代理，原因是多继承在 Java 中本质上就行不通。
Cglib动态代理的实现
​	JDK动态代理拥有局限性，那就是必须面向接口编程，没有接口就无法实现代理，我们也不可能为了代理而为每个需要实现代理的类强行添加毫无意义的接口，这时我们需要Cglib，这种依靠继承来实现动态代理的方式，不再要求我们必须要有接口。

17.dbcp c3p0的原理，优缺点
	C3P0是一个开源的JDBC连接池，它实现了数据源和JNDI绑定，支持JDBC3规范和JDBC2的标准扩展。目前使用它的开源项目有Hibernate，Spring等。
	DBCP(DataBase connection pool),数据库连接池。是 apache 上的一个 java 连接池项目，也是 tomcat 使用的连接池组件。
	c3p0与dbcp区别：
	dbcp没有自动的去回收空闲连接的功能 c3p0有自动回收空闲连接功能
	两者主要是对数据连接的处理方式不同！C3P0提供最大空闲时间，DBCP提供最大连接数。
	前者当连接超过最大空闲连接时间时，当前连接就会被断掉。DBCP当连接数超过最大连接数时，所有连接都会被断开。
	DBCP：比较稳定。
	C3P0: 性能比较高。

18.跨越请求处理的方法
	JSONP跨域
		JSONP（JSON with Padding）是数据格式JSON的一种“使用模式”，可以让网页从别的网域要数据。根据 XmlHttpRequest 对象受到同源策略的影响，而利用 <script>元素的这个开放策略，网页可以得到从其他来源动态产生的JSON数据，而这种使用模式就是所谓的 JSONP。用JSONP抓到的数据并不是JSON，而是任意的JavaScript，用 JavaScript解释器运行而不是用JSON解析器解析。所有，通过Chrome查看所有JSONP发送的Get请求都是js类型，而非XHR。
	CORS
		Cross-Origin Resource Sharing（CORS）跨域资源共享是一份浏览器技术的规范，提供了 Web 服务从不同域传来沙盒脚本的方法，以避开浏览器的同源策略，确保安全的跨域数据传输。现代浏览器使用CORS在API容器如XMLHttpRequest来减少HTTP请求的风险来源。与 JSONP 不同，CORS 除了 GET 要求方法以外也支持其他的 HTTP 要求。服务器一般需要增加如下响应头的一种或几种：
		Access-Control-Allow-Origin: *
		Access-Control-Allow-Methods: POST, GET, OPTIONS
		Access-Control-Allow-Headers: X-PINGOTHER, Content-Type
		Access-Control-Max-Age: 86400

		跨域请求默认不会携带Cookie信息，如果需要携带，请配置下述参数：
		"Access-Control-Allow-Credentials": true
		// Ajax设置
		"withCredentials": true
19.springMVC的原理
	首先，用户向服务器发送请求，请求被前端控制器DispatcherServet截获
　　DispatcherServlet没有业务处理的能力，调用HandlerMapping处理器映射器
　　HandlerMapping根据xml配置和注解找到具体的Handler，并将处理器执行链（包括Handler和拦截器数组）返回给DispatcherServlet
　　DispatcherServlet调用HandlerAdapter处理器适配器
　　HandlerAdapter经过适配器调用具体的处理器（Controller，也称后端控制器）
　　Controller执行完返回ModleAndView
　　HandlerAdapter将ModleAndView返回给DispatcherServlet
　　DispatcherServlet将ModleAndView传给ViewReslover视图解析器
　　ViewReslover解析后返回具体的View
　　Dispatcher根据View进行视图渲染（将Modle填充至View）
　　DispatcherServlet响应用户
20.spring的单例bean，并且是线程安全
	Spring容器中的bean默认是单例模式。当多个客户端同时请求一个服务时，容器会给每一个请求分配一个线程。这些线程会并发执行该请求对应的业务处理逻辑（成员方法），如果该处理逻辑中有对该单例bean状态的修改（体现为该单例bean的成员属性），则需要考虑线程同步问题。
	Spring使用ThreadLocal解决线程安全问题。一般情况下，只有无状态的Bean才可以在多线程环境下共享。Spring对一些Bean（如RequestContextHolder、TransactionSynchronizationManager、LocaleContextHolder等）中“非线程安全状态”，采用ThreadLocal进行处理，让它们也成为线程安全的状态，因此有状态的Bean也可以在多线程中共享了。
	ThreadLocal和线程同步机制（synchronized）都可以解决多线程中“相同变量”的访问冲突问题。
	线程同步机制（synchronized）通过“对象锁”，保证同一时间只有一个线程访问变量，让不同的线程排队访问。这时该变量是多个线程共享的，使用同步机制要求程序慎密地分析什么时候对变量进行读写，什么时候需要锁定某个对象，什么时候释放对象锁等繁杂的问题，程序设计和编写难度相对较大。
	ThreadLocal会为每一个线程提供一个独立的“变量副本”，从而隔离了多个线程对数据的访问冲突。因为每一个线程都拥有自己的变量副本，从而也就没有必要对该变量进行同步了。ThreadLocal提供了线程安全的共享对象，在编写多线程代码时，可以把不安全的变量封装进ThreadLocal。
	

如果某一进程中有多个线程在同时运行，而这些线程又同时运行某段代码。假如每次运行结果和单线程运行的结果是一样的，而且其他变量的值也和预期的是一样的，就是线程安全的。如果一个类所提供的方法，对于线程来说是原子操作，或者多个线程之间的切换不会导致该接口的执行结果存在二义性，那么便不用考虑同步的问题。
线程安全问题都是由全局变量及静态变量引起的。若每个线程中对全局变量、静态变量只有读操作，而无写操作，这个全局变量是线程安全的。若有多个线程同时执行写操作，则需要考虑线程同步，否则就可能影响线程安全。
① 常量始终是线程安全的，因为只存在读操作。
② 每次调用方法前都新建一个实例是线程安全的，因为不会访问共享的资源。
③ 局部变量是线程安全的。因为每执行一个方法，都会在独立的空间创建局部变量，它不是共享的资源。局部变量包括方法的参数变量和方法内变量。
有状态就是有数据存储功能。有状态对象(Stateful Bean)，就是有实例变量的对象，可以保存数据，是非线程安全的。在不同方法调用间不保留任何状态。
无状态就是一次操作，不能保存数据。无状态对象(Stateless Bean)，就是没有实例变量的对象，不能保存数据，是不变类，是线程安全的。

#### 21.hashMap的数据结构

​	众所周知，HashMap 是一个用于存储Key-Value键值对的集合，每一个键值对也叫做 Entry。
​	这些个键值对（Entry）分散存储在一个数组当中，这个数组就是HashMap的主干。
​	HashMap 数组每一个元素的初始值都是 Null。
​	已hash计算以后的下标做索引的数组里面，如果hash值一样就放在nextNode中
​	HashMap的默认长度是16 ，自动扩展或初始化时，长度必须是2的幂
​		目的：服务于从Key映射到index的Hash算法 
​		之前说过，从Key映射到HashMap数组的对应位置，会用到一个Hash函数：
​		index = Hash（“apple”）
​		如何实现一个尽量均匀分布的Hash函数呢？我们通过利用Key的HashCode值来做某种运算。
​		Hash算法的实现采用了位运算的方式 
​		如何进行位运算呢？有如下的公式（Length是HashMap的长度）：
​		index = HashCode（Key） & （Length - 
​		1） 
​		下面我们以值为“book”的 Key 来演示整个过程： 
​		1. 计算 book 的 hashcode，结果为十进制的 3029737，二进制的101110001110101110 1001。 
​		2. 假定 HashMap 长度是默认的16，计算Length-1的结果为十进制的15，二进制的1111。 
​		3. 把以上两个结果做与运算，101110001110101110 1001 & 1111 = 1001，十进制是9，所以 index=9。（与运算：和，同位上都为1则为1，否则为0
​		可以说，Hash 算法最终得到的 index 结果，完全取决于 Key 的 Hashcode 值的最后几位。

为什么长度必须是2的幂 ：Hash算法均匀分布的原则
HashMap扩容与Rehash

HashMap的容量是有限的。当经过多次元素插入，使得HashMap达到一定饱和度时，Key映射位置发生冲突的几率会逐渐提高。

这时候，HashMap需要扩展它的长度，也就是进行Resize。
影响发生Resize的因素有两个：
	1.Capacity
	HashMap的当前长度。上一期曾经说过，HashMap的长度是2的幂。
	2.LoadFactor
	HashMap负载因子，默认值为0.75f。
	衡量HashMap是否进行Resize的条件如下：
	HashMap.Size >= Capacity * LoadFactor
	HashMap的resize不是简单的把长度扩大，而是经历以下两个步骤 
	1.扩容
	创建一个新的Entry空数组，长度是原数组的2倍。
	2.ReHash
	遍历原Entry数组，把所有的Entry重新Hash到新数组。为什么要重新Hash呢？因为长度扩大以后，Hash的规则也随之改变。
	让我们回顾一下Hash公式：
	index = HashCode（Key） & （Length - 1）
	当原数组长度为8时，Hash运算是和111B做与运算；新数组长度为16，Hash运算是和1111B做与运算。Hash结果显然不同。
hash函数的原理
	这段代码叫“扰动函数”。题主贴的是Java 7的HashMap的源码，Java 8中这步已经简化了，只做一次16位右位移异或混合，而不是四次，但原理是不变的
https://blog.csdn.net/Jae_Peng/article/details/79562432

22.mysql 语句分析
	mysql优化（三）–explain分析sql语句执行效率
	Explain命令在解决数据库性能上是第一推荐使用命令，大部分的性能问题可以通过此命令来简单的解决，Explain可以用来查看SQL语句的执行效 果，可以帮助选择更好的索引和优化查询语句，写出更好的优化语句。
	Explain语法：explain select … from … [where …]
	例如：explain select * from news;
	输出：
	+----+-------------+-------+-------+-------------------+---------+---------+-------+------
	| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |
	+----+-------------+-------+-------+-------------------+---------+---------+-------+------
	下面对各个属性进行了解：
	1、id：这是SELECT的查询序列号
	2、select_type：select_type就是select的类型，可以有以下几种：
		SIMPLE：简单SELECT(不使用UNION或子查询等)
		PRIMARY：最外面的SELECT
		UNION：UNION中的第二个或后面的SELECT语句
		DEPENDENT UNION：UNION中的第二个或后面的SELECT语句，取决于外面的查询
		UNION RESULT：UNION的结果。
		SUBQUERY：子查询中的第一个SELECT
		DEPENDENT SUBQUERY：子查询中的第一个SELECT，取决于外面的查询
		DERIVED：导出表的SELECT(FROM子句的子查询)
	3、table：显示这一行的数据是关于哪张表的

4、type：这列最重要，显示了连接使用了哪种类别,有无使用索引，是使用Explain命令分析性能瓶颈的关键项之一。
	结果值从好到坏依次是：
	system > const > eq_ref > ref > fulltext > ref_or_null > index_merge > unique_subquery > index_subquery > range > index > ALL
	一般来说，得保证查询至少达到range级别，最好能达到ref，否则就可能会出现性能问题。

5、possible_keys：列指出MySQL能使用哪个索引在该表中找到行

6、key：显示MySQL实际决定使用的键（索引）。如果没有选择索引，键是NULL

7、key_len：显示MySQL决定使用的键长度。如果键是NULL，则长度为NULL。使用的索引的长度。在不损失精确性的情况下，长度越短越好

8、ref：显示使用哪个列或常数与key一起从表中选择行。

9、rows：显示MySQL认为它执行查询时必须检查的行数。数值越大越不好，说明没有用好索引

10、Extra：包含MySQL解决查询的详细信息，也是关键参考项之一。
	Distinct
	一旦MYSQL找到了与行相联合匹配的行，就不再搜索了
	Not exists
	MYSQL 优化了LEFT JOIN，一旦它找到了匹配LEFT JOIN标准的行，
	就不再搜索了
	Range checked for each
	Record（index map:#）
	没有找到理想的索引，因此对于从前面表中来的每一 个行组合，MYSQL检查使用哪个索引，并用它来从表中返回行。这是使用索引的最慢的连接之一
	Using filesort
	看 到这个的时候，查询就需要优化了。MYSQL需要进行额外的步骤来发现如何对返回的行排序。它根据连接类型以及存储排序键值和匹配条件的全部行的行指针来 排序全部行
	Using index
	列数据是从仅仅使用了索引中的信息而没有读取实际的行动的表返回的，这发生在对表 的全部的请求列都是同一个索引的部分的时候
	Using temporary
	看到这个的时候，查询需要优化了。这 里，MYSQL需要创建一个临时表来存储结果，这通常发生在对不同的列集进行ORDER BY上，而不是GROUP BY上
	Using where
	使用了WHERE从句来限制哪些行将与下一张表匹配或者是返回给用户。如果不想返回表中的全部行，并且连接类型ALL或index， 这就会发生，或者是查询有问题

其他一些Tip：

当type 显示为 “index” 时，并且Extra显示为“Using Index”， 表明使用了覆盖索引。

23.SpringMVC的扩展点
	SpirngMVC的第一个扩展点 
		HandlerMapping接口 -- 处理请求的映射
		保存请求url到具体的方法的映射关系，，我们可以编写任意的HandlerMapping实现类，依据任何策略来决定一个web请求到HandlerExecutionChain对象的生成。
	SpirngMVC的第二个扩展点
		  HandlerInterceptor 接口--拦截器
		  HandlerInterceptor，通过自定义拦截器，我们可以在一个请求被真正处理之前、请求被处理但还没输出到响应中、请求已经被输出到响应中之后这三个时间点去做任何我们想要做的事情。
	SpirngMVC的第三个扩展点
		HandlerAdapter
		真正调用
	SpirngMVC的第四个扩展点
		 HandlerMethodArgumentResolver -- 处理方法参数解释绑定器
		 调用controller方法之前，对方法参数进行解释绑定（实现WebArgumentResolver接口，spring3.1以后推荐使用HandlerMethodArgumentResolver）；
	SpirngMVC的第五个扩展点
		 Converter --  类型转换器
		 参数绑定时springmvc会对从前端传来的参数自动转化成方法定义的参数的类型，我们可自己定义此接口来实现自己的类型的转换
	SpirngMVC的第六个扩展点
		ViewResolver
		 完成从ModelAndView到真正的视图的过程，ViewResolver接口是在DispatcherServlet中进行调用的，当DispatcherServlet调用完Controller后，会得到一个ModelAndView对象，然后DispatcherServlet会调用render方法进行视图渲染。   
	SpringMVC提供的第七个扩展点：
		 HandlerExceptionResolver接口 --异常处理
24.单例的速度那个快（内部类最快）
25.类加载器
26.JVM调优
27.线程池的数量怎么定
28.多线程结果怎么返回
		Java主线程等待所有子线程执行完毕在执行，这个需求其实我们在工作中经常会用到，比如用户下单一个产品，后台会做一系列的处理，为了提高效率，每个处理都可以用一个线程来执行，所有处理完成了之后才会返回给用户下单成功。 
	我们通过以下的几种方法来解决： 
	一、用sleep方法，让主线程睡眠一段时间，当然这个睡眠时间是主观的时间，是我们自己定的，这个方法不推荐，但是在这里还是写一下，毕竟是解决方法 
	java主线程等待所有子线程执行完毕在执行（常见面试题） 
	二、使用Thread的join()等待所有的子线程执行完毕，主线程在执行，thread.join()把指定的线程加入到当前线程，可以将两个交替执行的线程合并为顺序执行的线程。比如在线程B中调用了线程A的Join()方法，直到线程A执行完毕后，才会继续执行线程B。 
	

```
/** 

- - 处理一个业务逻辑的场景：当一组线程都执行完之后，在执行别的线程（后者要使用前者返回的结果） 
  - @author Administrator 
  - */ 
    public class ThreadDemo {

public static void main(String[] args) throws InterruptedException {
	Vector<Thread> vectors=new Vector<Thread>();
	//启用5个线程
	for(int i=1;i<=5;i++){
		Thread childrenThread=new Thread(new Runnable(){
			 public void run(){
				 try {
					Thread.sleep(1000);
				} catch (Exception e) {
					e.printStackTrace();
				}
				 System.out.println("子线程执行！");

​			 }
​		});
​		vectors.add(childrenThread);
​		childrenThread.start();
​	}
​	//主线程
​	for(Thread thread : vectors){
​		thread.join(); //使用join来保证childrenThread的5个线程都执行完后，才执行主线程
​	}
​	System.out.println("主线程执行！");

}
} 
```

三、下面结合这个问题我介绍一些并发包里非常有用的并发工具类，等待多线程完成的CountDownLatch 

```
/** 

- 
- 处理一个业务逻辑的场景：当一组线程都执行完之后，在执行别的线程（后者要使用前者返回的结果） 
- @author Administrator 
- */ 
  public class ThreadDemo2 {

public static void main(String[] args) throws InterruptedException {
	final CountDownLatch latch= new CountDownLatch(5);//使用java并发库concurrent
	//启用5个线程
	for(int i=1;i<=5;i++){
		new Thread(new Runnable(){
			 public void run(){
				 try {
					Thread.sleep(1000);
				} catch (Exception e) {
					e.printStackTrace();
				}
				 System.out.println("子线程执行！");
				 latch.countDown();//让latch中的数值减一

​			 }
​		}).start();

​	}
​	//主线程
​	latch.await();//阻塞当前线程直到latch中数值为零才执行
​	System.out.println("主线程执行！");

}
} 
```

在这里说明一点，countDownLatch不可能重新初始化或者修改CountDownLatch对象内部计数器的值，一个线程调用countdown方法happen-before另外一个线程调用await方法 
四、同步屏障CyclicBarrier 

```
/** 

- 
- 处理一个业务逻辑的场景：当一组线程都执行完之后，在执行别的线程（后者要使用前者返回的结果） 
- @author Administrator 
- */ 
  public class ThreadDemo3 {

public static void main(String[] args) throws Exception {
	final CyclicBarrier barrier=new CyclicBarrier(5);
	//启用5个线程
	for(int i=1;i<=5;i++){
		new Thread(new Runnable(){
			 public void run(){
				 try {
					Thread.sleep(1000);
				} catch (Exception e) {
					e.printStackTrace();
				}
				 System.out.println("子线程执行！");
				 try {
					barrier.await();//到达屏障
				} catch (InterruptedException | BrokenBarrierException e) {
					// TODO Auto-generated catch block
					e.printStackTrace();
				}

​			 }
​		}).start();

​	}
​	//主线程
​	barrier.await();//阻塞当前线程直到latch中数值为零才执行
​	System.out.println("主线程执行！");

}
} 
```

写到这里大家不免有些疑问，countDownLatch和cyclicBarrier有什么区别呢，他们的区别：countDownLatch只能使用一次，而CyclicBarrier方法可以使用reset()方法重置，所以CyclicBarrier方法可以能处理更为复杂的业务场景。 
我曾经在网上看到一个关于countDownLatch和cyclicBarrier的形象比喻，就是在百米赛跑的比赛中若使用 countDownLatch的话冲过终点线一个人就给评委发送一个人的成绩，10个人比赛发送10次，如果用CyclicBarrier，则只在最后一个人冲过终点线的时候发送所有人的数据，仅仅发送一次，这就是区别。

写到这里大家不免有些疑问，countDownLatch和cyclicBarrier有什么区别呢，他们的区别：countDownLatch只能使用一次，而CyclicBarrier方法可以使用reset()方法重置，所以CyclicBarrier方法可以能处理更为复杂的业务场景。 
我曾经在网上看到一个关于countDownLatch和cyclicBarrier的形象比喻，就是在百米赛跑的比赛中若使用 countDownLatch的话冲过终点线一个人就给评委发送一个人的成绩，10个人比赛发送10次，如果用CyclicBarrier，则只在最后一个人冲过终点线的时候发送所有人的数据，仅仅发送一次，这就是区别。

29.队列的实现
	一、队列（Queue）
	队列是一种特殊的线性表，它只允许在表的前段（front）进行删除操作，只允许在表的后端（rear）进行插入操作。进行插入操作的端称为队尾，进行删除操作的端称为队头。
	对于一个队列来说，每个元素总是从队列的rear端进入队列，然后等待该元素之前的所有元素出队之后，当前元素才能出对，遵循先进先出（FIFO）原则。
	如果队列中不包含任何元素，该队列就被称为空队列。
	Java提供了一个Queue接口，并为该接口提供了众多的实现类：ArrayBlockingQueue、LinkedBlockingQueue、PriorityBlockingQueue、PriorityQueue、ConcurrentLinkedQueue和SynchronousQueue。
	其中常用的是：ArrayBlockingQueue、LinkedBlockingQueue和CurrentLinkedQueue，它们都是线程安全的队列。LinkedBlockingQueue队列的吞吐量通常比ArrayBlockingQueue队列高，但在大多数并发应用程序中，LinkedBlockingQueue的性能要低。
	除了LinkedBlockingQueue队列之外，JDK还提供了另外一种链队列ConcurrentLinkedQueue，它基于一种先进的、无等待（wait-free）队列算法实现。
	二、顺序队列存储结构的实现
	  1 package com.ietree.basic.datastructure.queue;
	  2 
	  3 import java.util.Arrays;
	  4 
	  5 /**
	  6  * Created by ietree
	  7  * 2017/4/29
	  8  */
	  9 public class SequenceQueue<T> {
	 10 
	 11     private int DEFAULT_SIZE = 10;
	 12     // 保存数组的长度
	 13     private int capacity;
	 14     // 定义一个数组用于保存顺序队列的元素
	 15     private Object[] elementData;
	 16     // 保存顺序队列中元素的当前个数
	 17     private int front = 0;
	 18     private int rear = 0;
	 19 
	 20     // 以默认数组长度创建空顺序队列
	 21     public SequenceQueue() {
	 22 
	 23         capacity = DEFAULT_SIZE;
	 24         elementData = new Object[capacity];
	 25 
	 26     }
	 27 
	 28     // 以一个初始化元素来创建顺序队列
	 29     public SequenceQueue(T element) {
	 30 
	 31         this();
	 32         elementData[0] = element;
	 33         rear++;
	 34 
	 35     }
	 36 
	 37     /**
	 38      * 以指定长度的数组来创建顺序线性表
	 39      *
	 40      * @param element  指定顺序队列中第一个元素
	 41      * @param initSize 指定顺序队列底层数组的长度
	 42      */
	 43     public SequenceQueue(T element, int initSize) {
	 44 
	 45         this.capacity = initSize;
	 46         elementData = new Object[capacity];
	 47         elementData[0] = element;
	 48         rear++;
	 49     }
	 50 
	 51     /**
	 52      * 获取顺序队列的大小
	 53      *
	 54      * @return 顺序队列的大小值
	 55      */
	 56     public int length() {
	 57 
	 58         return rear - front;
	 59 
	 60     }
	 61 
	 62     /**
	 63      * 插入队列
	 64      *
	 65      * @param element 入队列的元素
	 66      */
	 67     public void add(T element) {
	 68 
	 69         if (rear > capacity - 1) {
	 70             throw new IndexOutOfBoundsException("队列已满异常");
	 71         }
	 72         elementData[rear++] = element;
	 73 
	 74     }
	 75 
	 76     /**
	 77      * 移除队列
	 78      *
	 79      * @return 出队列的元素
	 80      */
	 81     public T remove() {
	 82 
	 83         if (empty()) {
	 84             throw new IndexOutOfBoundsException("空队列异常");
	 85         }
	 86 
	 87         // 保留队列的rear端的元素的值
	 88         T oldValue = (T) elementData[front];
	 89         // 释放队列顶元素
	 90         elementData[front++] = null;
	 91         return oldValue;
	 92 
	 93     }
	 94 
	 95     // 返回队列顶元素，但不删除队列顶元素
	 96     public T element() {
	 97 
	 98         if (empty()) {
	 99             throw new IndexOutOfBoundsException("空队列异常");
	100         }
	101         return (T) elementData[front];
	102 
	103     }
	104 
	105     // 判断顺序队列是否为空
	106     public boolean empty() {
	107 
	108         return rear == front;
	109 
	110     }
	111 
	112     // 清空顺序队列
	113     public void clear() {
	114 
	115         // 将底层数组所有元素赋值为null
	116         Arrays.fill(elementData, null);
	117         front = 0;
	118         rear = 0;
	119 
	120     }
	121 
	122     public String toString() {
	123 
	124         if (empty()) {
	125 
	126             return "[]";
	127 
	128         } else {
	129 
	130             StringBuilder sb = new StringBuilder("[");
	131             for (int i = front; i < rear; i++) {
	132                 sb.append(elementData[i].toString() + ", ");
	133             }
	134             int len = sb.length();
	135             return sb.delete(len - 2, len).append("]").toString();
	136         }
	137 
	138     }
	139 
	140 }
	测试类：
	 1 package com.ietree.basic.datastructure.queue;
	 2 
	 3 /**
	 4  * Created by ietree
	 5  * 2017/4/30
	 6  */
	 7 public class SequenceQueueTest {
	 8 
	 9     public static void main(String[] args) {
	10 
	11         SequenceQueue<String> queue = new SequenceQueue<String>();
	12         // 依次将4个元素加入到队列中
	13         queue.add("aaaa");
	14         queue.add("bbbb");
	15         queue.add("cccc");
	16         queue.add("dddd");
	17         System.out.println(queue);
	18 
	19         System.out.println("访问队列的front端元素：" + queue.element());
	20 
	21         System.out.println("第一次弹出队列的front端元素：" + queue.remove());
	22 
	23         System.out.println("第二次弹出队列的front端元素：" + queue.remove());
	24 
	25         System.out.println("两次remove之后的队列：" + queue);
	26     }
	27 
	28 }
	复制代码
	程序输出：

	[dddd, cccc, bbbb, aaaa]
	访问栈顶元素：dddd
	第一次弹出栈顶元素：dddd
	第二次弹出栈顶元素：cccc
	两次pop之后的栈：[bbbb, aaaa]
	三、队列的链式存储结构实现
	
	复制代码
	  1 package com.ietree.basic.datastructure.queue;
	  2 
	  3 /**
	  4  * Created by ietree
	  5  * 2017/4/30
	  6  */
	  7 public class LinkQueue<T> {
	  8 
	  9     // 定义一个内部类Node，Node实例代表链队列的节点
	 10     private class Node {
	 11 
	 12         // 保存节点的数据
	 13         private T data;
	 14         // 指向下个节点的引用
	 15         private Node next;
	 16 
	 17         // 无参构造器
	 18         public Node() {
	 19         }
	 20 
	 21         // 初始化全部属性的构造器
	 22         public Node(T data, Node next) {
	 23 
	 24             this.data = data;
	 25             this.next = next;
	 26 
	 27         }
	 28 
	 29     }
	 30 
	 31     // 保存该链队列的头节点
	 32     private Node front;
	 33     // 保存该链队列的尾节点
	 34     private Node rear;
	 35     // 保存该链队列中已包含的节点数
	 36     private int size;
	 37 
	 38     // 创建空链队列
	 39     public LinkQueue() {
	 40         // 空链队列，front和rear的值都为null
	 41         front = null;
	 42         rear = null;
	 43     }
	 44 
	 45     // 以指定数据元素来创建链队列，该链队列只有一个元素
	 46     public LinkQueue(T element) {
	 47 
	 48         front = new Node(element, null);
	 49         // 只有一个节点，front、rear都是指向该节点
	 50         rear = front;
	 51         size++;
	 52 
	 53     }
	 54 
	 55     // 返回链队列的长度
	 56     public int length() {
	 57 
	 58         return size;
	 59 
	 60     }
	 61 
	 62     // 将新元素加入队列
	 63     public void add(T element) {
	 64         // 如果该链队列还是空链队列
	 65         if (front == null) {
	 66             front = new Node(element, null);
	 67             // 只有一个节点，front、rear都是指向该节点
	 68             rear = front;
	 69         } else {
	 70             // 创建新节点
	 71             Node newNode = new Node(element, null);
	 72             // 让尾节点的next指向新增的节点
	 73             rear.next = newNode;
	 74             rear = newNode;
	 75         }
	 76         size++;
	 77     }
	 78 
	 79     // 删除队列front端的元素
	 80     public T remove() {
	 81 
	 82         Node oldfront = front;
	 83         // 让front引用指向原队列顶元素的下一个元素
	 84         front = front.next;
	 85         // 释放原队列顶元素的next引用
	 86         oldfront.next = null;
	 87         size--;
	 88         return oldfront.data;
	 89 
	 90     }
	 91 
	 92     // 访问队列顶元素，但不删除队列顶元素
	 93     public T element() {
	 94 
	 95         return rear.data;
	 96 
	 97     }
	 98 
	 99     // 判断链队列是否为空队列
	100     public boolean empty() {
	101 
	102         return size == 0;
	103 
	104     }
	105 
	106     // 请空链队列
	107     public void clear() {
	108         // 将front、rear两个节点赋为null
	109         front = null;
	110         rear = null;
	111         size = 0;
	112     }
	113 
	114     public String toString() {
	115 
	116         // 链队列为空队列时
	117         if (empty()) {
	118             return "[]";
	119         } else {
	120             StringBuilder sb = new StringBuilder("[");
	121             for (Node current = front; current != null; current = current.next) {
	122                 sb.append(current.data.toString() + ", ");
	123             }
	124             int len = sb.length();
	125             return sb.delete(len - 2, len).append("]").toString();
	126         }
	127 
	128     }
	129 
	130 }
	复制代码
	测试类：
	
	复制代码
	 1 package com.ietree.basic.datastructure.queue;
	 2 
	 3 /**
	 4  * Created by ietree
	 5  * 2017/4/30
	 6  */
	 7 public class LinkQueueTest {
	 8 
	 9     public static void main(String[] args) {
	10 
	11         LinkQueue<String> queue = new LinkQueue<String>("aaaa");
	12         // 依次将4个元素加入到队列中
	13         queue.add("bbbb");
	14         queue.add("cccc");
	15         queue.add("dddd");
	16         System.out.println(queue);
	17 
	18         // 删除一个元素后
	19         queue.remove();
	20         System.out.println("删除一个元素后的队列：" + queue);
	21 
	22         // 再添加一个元素
	23         queue.add("eeee");
	24         System.out.println("再次添加元素后的队列：" + queue);
	25 
	26     }
	27 
	28 }
	复制代码
	程序输出：
	
	[aaaa, bbbb, cccc, dddd]
	删除一个元素后的队列：[bbbb, cccc, dddd]
	再次添加元素后的队列：[bbbb, cccc, dddd, eeee]
30.hashcode怎么算
31.mybatis分页实现
32.线程安全的ConcurrentHashMap的实现
	HashTable容器在竞争激烈的并发环境下表现出效率低下的原因，是因为所有访问HashTable的线程都必须竞争同一把锁，那假如容器里有多把锁，每一把锁用于锁容器其中一部分数据，那么当多线程访问容器里不同数据段的数据时，线程间就不会存在锁竞争，从而可以有效的提高并发访问效率，这就是ConcurrentHashMap所使用的锁分段技术，首先将数据分成一段一段的存储，然后给每一段数据配一把锁，当一个线程占用锁访问其中一个段数据的时候，其他段的数据也能被其他线程访问。有些方法需要跨段，比如size()和containsValue()，它们可能需要锁定整个表而而不仅仅是某个段，这需要按顺序锁定所有段，操作完毕后，又按顺序释放所有段的锁。这里“按顺序”是很重要的，否则极有可能出现死锁，在ConcurrentHashMap内部，段数组是final的，并且其成员变量实际上也是final的，但是，仅仅是将数组声明为final的并不保证数组成员也是final的，这需要实现上的保证。这可以确保不会出现死锁，因为获得锁的顺序是固定的。
	ConcurrentHashMap是由Segment数组结构和HashEntry数组结构组成。Segment是一种可重入锁ReentrantLock，在ConcurrentHashMap里扮演锁的角色，HashEntry则用于存储键值对数据。一个ConcurrentHashMap里包含一个Segment数组，Segment的结构和HashMap类似，是一种数组和链表结构， 一个Segment里包含一个HashEntry数组，每个HashEntry是一个链表结构的元素， 每个Segment守护者一个HashEntry数组里的元素,当对HashEntry数组的数据进行修改时，必须首先获得它对应的Segment锁。
	应用场景
		当有一个大数组时需要在多个线程共享时就可以考虑是否把它给分层多个节点了，避免大锁。并可以考虑通过hash算法进行一些模块定位。
		其实不止用于线程，当设计数据表的事务时（事务某种意义上也是同步机制的体现），可以把一个表看成一个需要同步的数组，如果操作的表数据太多时就可以考虑事务分离了（这也是为什么要避免大表的出现），比如把数据进行字段拆分，水平分表等.
	源码解读
		ConcurrentHashMap(1.7及之前)中主要实体类就是三个：ConcurrentHashMap（整个Hash表）,Segment（桶），HashEntry（节点），对应上面的图可以看出之间的关系
	/** 
	* The segments, each of which is a specialized hash table 
	*/  
	final Segment<K,V>[] segments;
	不变(Immutable)和易变(Volatile)
		ConcurrentHashMap完全允许多个读操作并发进行，读操作并不需要加锁。如果使用传统的技术，如HashMap中的实现，如果允许可以在hash链的中间添加或删除元素，读操作不加锁将得到不一致的数据。ConcurrentHashMap实现技术是保证HashEntry几乎是不可变的。HashEntry代表每个hash链中的一个节点，其结构如下所示：
	 static final class HashEntry<K,V> {  
		 final K key;  
		 final int hash;  
		 volatile V value;  
		 final HashEntry<K,V> next;  
	 } 
	　　可以看到除了value不是final的，其它值都是final的，这意味着不能从hash链的中间或尾部添加或删除节点，因为这需要修改next 引用值，所有的节点的修改只能从头部开始。对于put操作，可以一律添加到Hash链的头部。但是对于remove操作，可能需要从中间删除一个节点，这就需要将要删除节点的前面所有节点整个复制一遍，最后一个节点指向要删除结点的下一个结点。这在讲解删除操作时还会详述。为了确保读操作能够看到最新的值，将value设置成volatile，这避免了加锁。
	其它
	　　为了加快定位段以及段中hash槽的速度，每个段hash槽的的个数都是2^n，这使得通过位运算就可以定位段和段中hash槽的位置。当并发级别为默认值16时，也就是段的个数，hash值的高4位决定分配在哪个段中。但是我们也不要忘记《算法导论》给我们的教训：hash槽的的个数不应该是 2^n，这可能导致hash槽分配不均，这需要对hash值重新再hash一次。（这段似乎有点多余了 ）
	 
	定位操作：
	 final Segment<K,V> segmentFor(int hash) {  
		 return segments[(hash >>> segmentShift) & segmentMask];  
	 }
	　　既然ConcurrentHashMap使用分段锁Segment来保护不同段的数据，那么在插入和获取元素的时候，必须先通过哈希算法定位到Segment。可以看到ConcurrentHashMap会首先使用Wang/Jenkins hash的变种算法对元素的hashCode进行一次再哈希。
	再哈希，其目的是为了减少哈希冲突，使元素能够均匀的分布在不同的Segment上，从而提高容器的存取效率。假如哈希的质量差到极点，那么所有的元素都在一个Segment中，不仅存取元素缓慢，分段锁也会失去意义。我做了一个测试，不通过再哈希而直接执行哈希计算。
	 
	System.out.println(Integer.parseInt("0001111", 2) & 15);
	System.out.println(Integer.parseInt("0011111", 2) & 15);
	System.out.println(Integer.parseInt("0111111", 2) & 15);
	System.out.println(Integer.parseInt("1111111", 2) & 15);
	 
	计算后输出的哈希值全是15，通过这个例子可以发现如果不进行再哈希，哈希冲突会非常严重，因为只要低位一样，无论高位是什么数，其哈希值总是一样。我们再把上面的二进制数据进行再哈希后结果如下，为了方便阅读，不足32位的高位补了0，每隔四位用竖线分割下。
	 
	0100｜0111｜0110｜0111｜1101｜1010｜0100｜1110
	1111｜0111｜0100｜0011｜0000｜0001｜1011｜1000
	0111｜0111｜0110｜1001｜0100｜0110｜0011｜1110
	1000｜0011｜0000｜0000｜1100｜1000｜0001｜1010
	 
	可以发现每一位的数据都散列开了，通过这种再哈希能让数字的每一位都能参加到哈希运算当中，从而减少哈希冲突。ConcurrentHashMap通过以下哈希算法定位segment。
	默认情况下segmentShift为28，segmentMask为15，再哈希后的数最大是32位二进制数据，向右无符号移动28位，意思是让高4位参与到hash运算中， (hash >>> segmentShift) & segmentMask的运算结果分别是4，15，7和8，可以看到hash值没有发生冲突。
	final Segment<K,V> segmentFor(int hash) {
		return segments[(hash >>> segmentShift) & segmentMask];
	}

数据结构

　　所有的成员都是final的，其中segmentMask和segmentShift主要是为了定位段，参见上面的segmentFor方法。
　　关于Hash表的基础数据结构，这里不想做过多的探讨。Hash表的一个很重要方面就是如何解决hash冲突，ConcurrentHashMap 和HashMap使用相同的方式，都是将hash值相同的节点放在一个hash链中。与HashMap不同的是，ConcurrentHashMap使用多个子Hash表，也就是段(Segment)。
每个Segment相当于一个子Hash表，它的数据成员如下：
复制代码
static final class Segment<K,V> extends ReentrantLock implements Serializable {    
		 /** 
		  * The number of elements in this segment's region. 
		  */
		 transient volatileint count;  
		 /** 
		  * Number of updates that alter the size of the table. This is 
		  * used during bulk-read methods to make sure they see a 
		  * consistent snapshot: If modCounts change during a traversal 
		  * of segments computing size or checking containsValue, then 
		  * we might have an inconsistent view of state so (usually) 
		  * must retry. 
		  */
		 transient int modCount;  
		 /** 
		  * The table is rehashed when its size exceeds this threshold. 
		  * (The value of this field is always <tt>(int)(capacity * 
		  * loadFactor)</tt>.) 
		  */
		 transient int threshold;  
		 /** 
		  * The per-segment table. 
		  */
		 transient volatile HashEntry<K,V>[] table;  
		 /** 
		  * The load factor for the hash table.  Even though this value 
		  * is same for all segments, it is replicated to avoid needing 
		  * links to outer object. 
		  * @serial 
		  */
		 final float loadFactor;  
 } 
复制代码
　　count用来统计该段数据的个数，它是volatile，它用来协调修改和读取操作，以保证读取操作能够读取到几乎最新的修改。协调方式是这样的，每次修改操作做了结构上的改变，如增加/删除节点(修改节点的值不算结构上的改变)，都要写count值，每次读取操作开始都要读取count的值。这利用了 Java 5中对volatile语义的增强，对同一个volatile变量的写和读存在happens-before关系。modCount统计段结构改变的次数，主要是为了检测对多个段进行遍历过程中某个段是否发生改变，在讲述跨段操作时会还会详述。threashold用来表示需要进行rehash的界限值。table数组存储段中节点，每个数组元素是个hash链，用HashEntry表示。table也是volatile，这使得能够读取到最新的 table值而不需要同步。loadFactor表示负载因子。
删除操作remove(key)
public V remove(Object key) {  
         hash = hash(key.hashCode());   
         return segmentFor(hash).remove(key, hash, null);   
}

   整个操作是先定位到段，然后委托给段的remove操作。当多个删除操作并发进行时，只要它们所在的段不相同，它们就可以同时进行。
下面是Segment的remove方法实现：
复制代码
 V remove(Object key, int hash, Object value) {  
	 lock();  
	 try {  
		 int c = count - 1;  
		 HashEntry<K,V>[] tab = table;  
		 int index = hash & (tab.length - 1);  
		 HashEntry<K,V> first = tab[index];  
		 HashEntry<K,V> e = first;  
		 while (e != null && (e.hash != hash || !key.equals(e.key)))  
			 e = e.next;  
		 V oldValue = null;  
		 if (e != null) {  
			 V v = e.value;  
			 if (value == null || value.equals(v)) {  
				 oldValue = v;  

​				 // All entries following removed node can stay  
​				 // in list, but all preceding ones need to be  
​				 // cloned.  
​				 ++modCount;  
​				 HashEntry<K,V> newFirst = e.next;  
​				 *for (HashEntry<K,V> p = first; p != e; p = p.next)  
​					 newFirst = new HashEntry<K,V>(p.key, p.hash,  
​												   newFirst, p.value);  
​				 tab[index] = newFirst;  
​				 count = c; // write-volatile  
​			 }  
​		 }  
​		 return oldValue;  
​	 } finally {  
​		 unlock();  
​	 }  
 }
复制代码
　　整个操作是在持有段锁的情况下执行的，空白行之前的行主要是定位到要删除的节点e。接下来，如果不存在这个节点就直接返回null，否则就要将e前面的结点复制一遍，尾结点指向e的下一个结点。e后面的结点不需要复制，它们可以重用。
中间那个for循环是做什么用的呢？（号标记）从代码来看，就是将定位之后的所有entry克隆并拼回前面去，但有必要吗？每次删除一个元素就要将那之前的元素克隆一遍？这点其实是由entry的不变性来决定的，仔细观察entry定义，发现除了value，其他所有属性都是用final来修饰的，这意味着在第一次设置了next域之后便不能再改变它，取而代之的是将它之前的节点全都克隆一次。至于entry为什么要设置为不变性，这跟不变性的访问不需要同步从而节省时间有关
下面是个示意图
　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　删除元素之前：


​	 
​	 
​	　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　
​	　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　删除元素3之后：


​	 
​	 
​	　　第二个图其实有点问题，复制的结点中应该是值为2的结点在前面，值为1的结点在后面，也就是刚好和原来结点顺序相反，还好这不影响我们的讨论。
​	整个remove实现并不复杂，但是需要注意如下几点。第一，当要删除的结点存在时，删除的最后一步操作要将count的值减一。这必须是最后一步操作，否则读取操作可能看不到之前对段所做的结构性修改。第二，remove执行的开始就将table赋给一个局部变量tab，这是因为table是 volatile变量，读写volatile变量的开销很大。编译器也不能对volatile变量的读写做任何优化，直接多次访问非volatile实例变量没有多大影响，编译器会做相应优化。
​	get操作
​	ConcurrentHashMap的get操作是直接委托给Segment的get方法，直接看Segment的get方法：
​	复制代码
​	V get(Object key, int hash) {  
​		 if (count != 0) { // read-volatile 当前桶的数据个数是否为0 
​			 HashEntry<K,V> e = getFirst(hash);  得到头节点
​			 while (e != null) {  
​				 if (e.hash == hash && key.equals(e.key)) {  
​					 V v = e.value;  
​					 if (v != null)  
​						 return v;  
​					 return readValueUnderLock(e); // recheck  
​				 }  
​				 e = e.next;  
​			 }  
​		 }  
​		 returnnull;  
​	 } 
​	复制代码
​	get操作不需要锁。
​	　　除非读到的值是空的才会加锁重读，我们知道HashTable容器的get方法是需要加锁的，那么ConcurrentHashMap的get操作是如何做到不加锁的呢？原因是它的get方法里将要使用的共享变量都定义成volatile
​	 
​	　　第一步是访问count变量，这是一个volatile变量，由于所有的修改操作在进行结构修改时都会在最后一步写count 变量，通过这种机制保证get操作能够得到几乎最新的结构更新。对于非结构更新，也就是结点值的改变，由于HashEntry的value变量是 volatile的，也能保证读取到最新的值。
​	 
	　　接下来就是根据hash和key对hash链进行遍历找到要获取的结点，如果没有找到，直接访回null。对hash链进行遍历不需要加锁的原因在于链指针next是final的。但是头指针却不是final的，这是通过getFirst(hash)方法返回，也就是存在 table数组中的值。这使得getFirst(hash)可能返回过时的头结点，例如，当执行get方法时，刚执行完getFirst(hash)之后，另一个线程执行了删除操作并更新头结点，这就导致get方法中返回的头结点不是最新的。这是可以允许，通过对count变量的协调机制，get能读取到几乎最新的数据，虽然可能不是最新的。要得到最新的数据，只有采用完全的同步。
	 
	　　最后，如果找到了所求的结点，判断它的值如果非空就直接返回，否则在有锁的状态下再读一次。这似乎有些费解，理论上结点的值不可能为空，这是因为 put的时候就进行了判断，如果为空就要抛NullPointerException。空值的唯一源头就是HashEntry中的默认值，因为 HashEntry中的value不是final的，非同步读取有可能读取到空值。仔细看下put操作的语句：tab[index] = new HashEntry<K,V>(key, hash, first, value)，在这条语句中，HashEntry构造函数中对value的赋值以及对tab[index]的赋值可能被重新排序，这就可能导致结点的值为空。这里当v为空时，可能是一个线程正在改变节点，而之前的get操作都未进行锁定，根据bernstein条件，读后写或写后读都会引起数据的不一致，所以这里要对这个e重新上锁再读一遍，以保证得到的是正确值。
	复制代码
	 V readValueUnderLock(HashEntry<K,V> e) {  
		 lock();  
		 try {  
			 return e.value;  
		 } finally {  
			 unlock();  
		 }  
	 }
	复制代码
	　　如用于统计当前Segement大小的count字段和用于存储值的HashEntry的value。定义成volatile的变量，能够在线程之间保持可见性，能够被多线程同时读，并且保证不会读到过期的值，但是只能被单线程写（有一种情况可以被多线程写，就是写入的值不依赖于原值），在get操作里只需要读不需要写共享变量count和value，所以可以不用加锁。之所以不会读到过期的值，是根据java内存模型的happen before原则，对volatile字段的写入操作先于读操作，即使两个线程同时修改和获取volatile变量，get操作也能拿到最新的值，这是用volatile替换锁的经典应用场景
	put操作
	同样地put操作也是委托给段的put方法。下面是段的put方法：
	复制代码
	 V put(K key, int hash, V value, boolean onlyIfAbsent) {  
		 lock();  
		 try {  
			 int c = count;  
			 if (c++ > threshold) // ensure capacity  
				 rehash();  
			 HashEntry<K,V>[] tab = table;  
			 int index = hash & (tab.length - 1);  
			 HashEntry<K,V> first = tab[index];  
			 HashEntry<K,V> e = first;  
			 while (e != null && (e.hash != hash || !key.equals(e.key)))  
				 e = e.next;  
			 V oldValue;  
			 if (e != null) {  
				 oldValue = e.value;  
				 if (!onlyIfAbsent)  
					 e.value = value;  
			 }  
			 else {  
				 oldValue = null;  
				 ++modCount;  
				 tab[index] = new HashEntry<K,V>(key, hash, first, value);  
				 count = c; // write-volatile  
			 }  
			 return oldValue;  
		 } finally {  
			 unlock();  
		 }  
	 }
	复制代码
	　　该方法也是在持有段锁(锁定整个segment)的情况下执行的，这当然是为了并发的安全，修改数据是不能并发进行的，必须得有个判断是否超限的语句以确保容量不足时能够rehash。接着是找是否存在同样一个key的结点，如果存在就直接替换这个结点的值。否则创建一个新的结点并添加到hash链的头部，这时一定要修改modCount和count的值，同样修改count的值一定要放在最后一步。put方法调用了rehash方法，reash方法实现得也很精巧，主要利用了table的大小为2^n，这里就不介绍了。而比较难懂的是这句int index = hash & (tab.length - 1)，原来segment里面才是真正的hashtable，即每个segment是一个传统意义上的hashtable,如上图，从两者的结构就可以看出区别，这里就是找出需要的entry在table的哪一个位置，之后得到的entry就是这个链的第一个节点，如果e!=null，说明找到了，这是就要替换节点的值（onlyIfAbsent == false），否则，我们需要new一个entry，它的后继是first，而让tab[index]指向它，什么意思呢？实际上就是将这个新entry插入到链头，剩下的就非常容易理解了
	 
	　　由于put方法里需要对共享变量进行写入操作，所以为了线程安全，在操作共享变量时必须得加锁。Put方法首先定位到Segment，然后在Segment里进行插入操作。插入操作需要经历两个步骤，第一步判断是否需要对Segment里的HashEntry数组进行扩容，第二步定位添加元素的位置然后放在HashEntry数组里。
	是否需要扩容。在插入元素前会先判断Segment里的HashEntry数组是否超过容量（threshold），如果超过阀值，数组进行扩容。值得一提的是，Segment的扩容判断比HashMap更恰当，因为HashMap是在插入元素后判断元素是否已经到达容量的，如果到达了就进行扩容，但是很有可能扩容之后没有新元素插入，这时HashMap就进行了一次无效的扩容。
	如何扩容。扩容的时候首先会创建一个两倍于原容量的数组，然后将原数组里的元素进行再hash后插入到新的数组里。为了高效ConcurrentHashMap不会对整个容器进行扩容，而只对某个segment进行扩容。
	 
	另一个操作是containsKey，这个实现就要简单得多了，因为它不需要读取值：
	 
	复制代码
	 boolean containsKey(Object key, int hash) {  
		 if (count != 0) { // read-volatile  
			 HashEntry<K,V> e = getFirst(hash);  
			 while (e != null) {  
				 if (e.hash == hash && key.equals(e.key))  
					 returntrue;  
				 e = e.next;  
			 }  
		 }  
		 returnfalse;  
	 } 
	复制代码
	size()操作
	　　如果我们要统计整个ConcurrentHashMap里元素的大小，就必须统计所有Segment里元素的大小后求和。Segment里的全局变量count是一个volatile变量，那么在多线程场景下，我们是不是直接把所有Segment的count相加就可以得到整个ConcurrentHashMap大小了呢？不是的，虽然相加时可以获取每个Segment的count的最新值，但是拿到之后可能累加前使用的count发生了变化，那么统计结果就不准了。所以最安全的做法，是在统计size的时候把所有Segment的put，remove和clean方法全部锁住，但是这种做法显然非常低效。
	　　因为在累加count操作过程中，之前累加过的count发生变化的几率非常小，所以ConcurrentHashMap的做法是先尝试2次通过不锁住Segment的方式来统计各个Segment大小，如果统计的过程中，容器的count发生了变化，则再采用加锁的方式来统计所有Segment的大小。
	　　那么ConcurrentHashMap是如何判断在统计的时候容器是否发生了变化呢？使用modCount变量，在put , remove和clean方法里操作元素前都会将变量modCount进行加1，那么在统计size前后比较modCount是否发生变化，从而得知容器的大小是否发生变化。
	http://www.cnblogs.com/ITtangtang/p/3948786.html
	https://www.cnblogs.com/zhaojj/p/8942647.html
33.jvm结构
34.StackOverflowError OutOfMemoryError出现的例子
	1.StackOverflowError
	　　堆栈溢出错误一般是递归调用嘛。下面的代码就可以出现：
	package T20131009;
	public class StackOverflowTest {
		public static void main(String[] args) {
			method();
		}
		public static void method(){
			for(;;)
				method();
		}
	}
	运行结果：抛出异常
	 2.OutOfMemoryError
	 　　内存溢出一般是出现在申请了较多的内存空间没有释放的情形。下面的代码就可以出现：
	package T20131009;
	import java.util.ArrayList;
	import java.util.List;
	public class OutOfMemoryTest {
		public static void main(String[] args){
			List list=new ArrayList();
			for(;;){
				int[] tmp=new int[1000000];
				list.add(tmp);
			}
		}
	}
	抛出异常
35.数据库索引的存储
36.redis怎么存数据的
37.String、StringBuffer、StringBuilder的不变性理解
	都是final类，都不允许被继承
	String长度不可变，而StringBuffer、StringBuild是可变的。主要原因如下：

	String类中包含一个不可变的char数组value用来存放字符串存储数据的变量：
	
	  /** The value is used for character storage. */
		private final char value[];
	1
	2
	而StringBuffer的构造函数中，使用super调用父类AbstractStringBuilder类中的方法:
	
	 /**
		 * Constructs a string buffer with no characters in it and an
		initial capacity of 16 characters.
		 */
		public StringBuffer() {
			super(16);
		}
	AbstractStringBuilder类:
	
		 /* The value is used for character storage.
		 */
		char[] value;
		/**
		 * Creates an AbstractStringBuilder of the specified capacity.
		 */
		AbstractStringBuilder(int capacity) {
			value = new char[capacity];
		}
	AbstractStringBuilder类的char数组并不是final类型，故长度可变。
	StringBuffer、StringBuilder两个中的所有方法都是相同的，但是StringBuffer添加了synchronized修饰，保证线程安全，而StringBuilder不是线程安全的，所以StringBuilder拥有更好的性能；
	在java中，String使用“+”符号来拼接字符串时，底层都会使用StringBuild实例的append()方法来实现。StringBuffer不能使用“+”拼接字符串
38.hashcode（）和equals（）的作用、区别、联系
	hashCode()方法和equal()方法的作用其实一样，在Java里都是用来对比两个对象是否相等一致，那么equal()既然已经能实现对比的功能了，为什么还要hashCode()呢？
		因为重写的equal（）里一般比较的比较全面比较复杂，这样效率就比较低，而利用hashCode()进行对比，则只要生成一个hash值进行比较就可以了，效率很高，那么hashCode()既然效率这么高为什么还要equal()呢？
			   因为hashCode()并不是完全可靠，有时候不同的对象他们生成的hashcode也会一样（生成hash值得公式可能存在的问题），所以hashCode()只能说是大部分时候可靠，并不是绝对可靠，所以我们可以得出：
			 1.equal()相等的两个对象他们的hashCode()肯定相等，也就是用equal()对比是绝对可靠的。
			 2.hashCode()相等的两个对象他们的equal()不一定相等，也就是hashCode()不是绝对可靠的。
	所有对于需要大量并且快速的对比的话如果都用equal()去做显然效率太低，所以解决方式是，每当需要对比的时候，首先用hashCode()去对比，如果hashCode()不一样，则表示这两个对象肯定不相等（也就是不必再用equal()去再对比了）,如果hashCode()相同，此时再对比他们的equal()，如果equal()也相同，则表示这两个对象是真的相同了，这样既能大大提高了效率也保证了对比的绝对正确性！
		 这种大量的并且快速的对象对比一般使用的hash容器中，比如hashset,hashmap,hashtable等等，比如hashset里要求对象不能重复，则他内部必然要对添加进去的每个对象进行对比，而他的对比规则就是像上面说的那样，先hashCode()，如果hashCode()相同，再用equal()验证，如果hashCode()都不同，则肯定不同，这样对比的效率就很高了。
		  然而hashCode()和equal()一样都是基本类Object里的方法，而和equal()一样，Object里hashCode()里面只是返回当前对象的地址，如果是这样的话，那么我们相同的一个类，new两个对象，由于他们在内存里的地址不同，则他们的hashCode（）不同，所以这显然不是我们想要的，所以我们必须重写我们类的hashCode()方法，即一个类，在hashCode()里面返回唯一的一个hash值，比如下面：
	自定义一个类 
	class Person{

	   int num;
	
	   String name;


​	 

	   public int hashCode(){
	
		  return num*name.hashCode();
	
	}
	
	}
	由于标识这个类的是他的内部的变量num和name,所以我们就根据他们返回一个hash值，作为这个类的唯一hash值。	 
	所以如果我们的对象要想放进hashSet，并且发挥hashSet的特性（即不包含一样的对象），则我们就要重写我们类的hashCode()和
	equal()方法了。像String,Integer等这种类内部都已经重写了这两个方法。
	当然如果我们只是平时想对比两个对象 是否一致，则只重写一个equal()，然后利用equal()去对比也行的。

39.生产者-消费者Java实现
	1 概述

	生产者消费者问题是多线程的一个经典问题，它描述是有一块缓冲区作为仓库，生产者可以将产品放入仓库，消费者则可以从仓库中取走产品。
	
	解决生产者/消费者问题的方法可分为两类：
	
	采用某种机制保护生产者和消费者之间的同步；
	在生产者和消费者之间建立一个管道。
	第一种方式有较高的效率，并且易于实现，代码的可控制性较好，属于常用的模式。第二种管道缓冲区不易控制，被传输数据对象不易于封装等，实用性不强。
	
	在Java中有四种方法支持同步，其中前三个是同步方法，一个是管道方法。
	
	wait() / notify()方法
	await() / signal()方法
	BlockingQueue阻塞队列方法
	PipedInputStream / PipedOutputStream
	本文只介绍前三种。
	
	2 实现


	2.1 wait() / notify()方法
	
	wait() / nofity()方法是基类Object的两个方法：
	
	wait()方法：当缓冲区已满/空时，生产者/消费者线程停止自己的执行，放弃锁，使自己处于等等状态，让其他线程执行。
	notify()方法：当生产者/消费者向缓冲区放入/取出一个产品时，向其他等待的线程发出可执行的通知，同时放弃锁，使自己处于等待状态。
	缓冲区Storage.java代码如下：


	复制代码
	import java.util.LinkedList;
	
	public class Storage
	{
		// 仓库最大存储量
		private final int MAX_SIZE = 100;
	
		// 仓库存储的载体
		private LinkedList<Object> list = new LinkedList<Object>();
	
		// 生产产品
		public void produce(String producer)
		{
			synchronized (list)
			{
				// 如果仓库已满
				while (list.size() == MAX_SIZE)
				{
					System.out.println("仓库已满，【"+producer+"】： 暂时不能执行生产任务!");
					try
					{
						// 由于条件不满足，生产阻塞
						list.wait();
					}
					catch (InterruptedException e)
					{
						e.printStackTrace();
					}
				}
	
				// 生产产品            
				list.add(new Object());            
	
				System.out.println("【"+producer+"】：生产了一个产品\t【现仓储量为】:" + list.size());
	
				list.notifyAll();
			}
		}
	
		// 消费产品
		public void consume(String consumer)
		{
			synchronized (list)
			{
				//如果仓库存储量不足
				while (list.size()==0)
				{
					System.out.println("仓库已空，【"+consumer+"】： 暂时不能执行消费任务!");
					try
					{
						// 由于条件不满足，消费阻塞
						list.wait();
					}
					catch (InterruptedException e)
					{
						e.printStackTrace();
					}
				}
				
				list.remove();
				System.out.println("【"+consumer+"】：消费了一个产品\t【现仓储量为】:" + list.size());
				list.notifyAll();
			}
		}
	
		public LinkedList<Object> getList()
		{
			return list;
		}
	
		public void setList(LinkedList<Object> list)
		{
			this.list = list;
		}
	
		public int getMAX_SIZE()
		{
			return MAX_SIZE;
		}
	}
	复制代码
	Test.java
	public class Test {
		public static void main(String[] args)
		{
			Storage storage=new Storage();
	
			for(int i=1;i<6;i++)
			{
				int finalI = i;
				new Thread(new Runnable() {
					@Override
					public void run() {
						storage.produce(String.format("生成者%d:", finalI));
					}
				}).start();
			}
	
			for(int i=1;i<4;i++)
			{
				int finalI = i;
				new Thread(()-> storage.consume(String.format("消费者%d:", finalI))).start();
			}
		}
	}
	 View Code
	结果如下：
	
	复制代码
	仓库已空，【消费者1】： 暂时不能执行消费任务!
	【生产者3】：生产了一个产品    【现仓储量为】:1
	【消费者2】：消费了一个产品    【现仓储量为】:0
	仓库已空，【消费者3】： 暂时不能执行消费任务!
	【生产者1】：生产了一个产品    【现仓储量为】:1
	【生产者4】：生产了一个产品    【现仓储量为】:2
	【生产者2】：生产了一个产品    【现仓储量为】:3
	【生产者5】：生产了一个产品    【现仓储量为】:4
	【消费者1】：消费了一个产品    【现仓储量为】:3
	【消费者3】：消费了一个产品    【现仓储量为】:2


​	
40.spring事务的传播特性
​	传播行为
​	含义
​	propagation_required（xml文件中为required)
​		表示当前方法必须在一个具有事务的上下文中运行，如有客户端有事务在进行，那么被调用端将在该事务中运行，否则的话重新开启一个事务。（如果被调用端发生异常，那么调用端和被调用端事务都将回滚）
​	propagation_supports(xml文件中为supports）
​		表示当前方法不必需要具有一个事务上下文，但是如果有一个事务的话，它也可以在这个事务中运行
​	propagation_mandatory(xml文件中为mandatory）
​		表示当前方法必须在一个事务中运行，如果没有事务，将抛出异常
​	propagation_nested(xml文件中为nested)
​		表示如果当前方法正有一个事务在运行中，则该方法应该运行在一个嵌套事务中，被嵌套的事务可以独立于被封装的事务中进行提交或者回滚。如果封装事务存在，并且外层事务抛出异常回滚，那么内层事务必须回滚，反之，内层事务并不影响外层事务。如果封装事务不存在，则同propagation_required的一样
​	propagation_never（xml文件中为never)
​		表示当方法务不应该在一个事务中运行，如果存在一个事务，则抛出异常
​	propagation_requires_new(xml文件中为requires_new）
​		表示当前方法必须运行在它自己的事务中。一个新的事务将启动，而且如果有一个现有的事务在运行的话，则这个方法将在运行期被挂起，直到新的事务提交或者回滚才恢复执行。
​	propagation_not_supported（xml文件中为not_supported）
​		表示该方法不应该在一个事务中运行。如果有一个事务正在运行，他将在运行期被挂起，直到这个事务提交或者回滚才恢复执行
41.spring事务的隔离级别和MySQL的事务的隔离级别

	mysql事务的并发问题
	　　1、脏读：事务A读取了事务B更新的数据，然后B回滚操作，那么A读取到的数据是脏数据
	　　2、不可重复读：事务 A 多次读取同一数据，事务 B 在事务A多次读取的过程中，对数据作了更新并提交，导致事务A多次读取同一数据时，结果 不一致。
	　　3、幻读：系统管理员A将数据库中所有学生的成绩从具体分数改为ABCDE等级，但是系统管理员B就在这个时候插入了一条具体分数的记录，当系统管理员A改结束后发现还有一条记录没有改过来，就好像发生了幻觉一样，这就叫幻读。
	　　小结：不可重复读的和幻读很容易混淆，不可重复读侧重于修改，幻读侧重于新增或删除。解决不可重复读的问题只需锁住满足条件的行，解决幻读需要锁表
	
	MySQL事务隔离级别
	
	事务隔离级别					脏读	不可重复读	幻读
	读未提交（read-uncommitted）	是		是			是
	不可重复读（read-committed）	否		是			是
	可重复读（repeatable-read）		否		否			是
	串行化（serializable）			否		否			否
	
	Spring五个事务隔离级别和七个事务传播行为
	
	1. 脏读 ：脏读就是指当一个事务正在访问数据，并且对数据进行了修改，而这种修改还没有提交到数据库中，这时，另外一个事务也访问这个数据，然后使用了这个数据。
	
	2. 不可重复读 ：是指在一个事务内，多次读同一数据。在这个事务还没有结束时，另外一个事务也访问该同一数据。那么，在第一个事务中的两 次读数据之间，由于第二个事务的修改，那么第一个事务两次读到的的数据可能是不一样的。这样就发生了在一个事务内两次读到的数据是不一样的，因此称为是不 可重复读。例如，一个编辑人员两次读取同一文档，但在两次读取之间，作者重写了该文档。当编辑人员第二次读取文档时，文档已更改。原始读取不可重复。如果 只有在作者全部完成编写后编辑人员才可以读取文档，则可以避免该问题。spacer.gif …数据库事务和Spring事务是一般面试都会被提到，很多朋友写惯了代码，很少花时间去整理归纳这些东西，结果本来会的东西，居然吞吞吐吐答不上来。
	
	下面是我收集到一些关于Spring事务的问题，希望能帮助大家过关。
	
	3. 幻读 : 是指当事务不是独立执行时发生的一种现象，例如第一个事务对一个表中的数据进行了修改，这种修改涉及到表中的全部数据行。 同时，第二个事务也修改这个表中的数据，这种修改是向表中插入一行新数据。那么，以后就会发生操作第一个事务的用户发现表中还有没有修改的数据行，就好象 发生了幻觉一样。例如，一个编辑人员更改作者提交的文档，但当生产部门将其更改内容合并到该文档的主复本时，发现作者已将未编辑的新材料添加到该文档中。 如果在编辑人员和生产部门完成对原始文档的处理之前，任何人都不能将新材料添加到文档中，则可以避免该问题。
	
	补充 : 基于元数据的 Spring 声明性事务 :
	
	Isolation 属性一共支持五种事务设置，具体介绍如下：
	
	<!—->l          <!—->DEFAULT 使用数据库设置的隔离级别 ( 默认 ) ，由 DBA 默认的设置来决定隔离级别 .
	
	<!—->l          <!—->READ_UNCOMMITTED 会出现脏读、不可重复读、幻读 ( 隔离级别最低，并发性能高 )
	
	<!—->l          <!—->READ_COMMITTED  会出现不可重复读、幻读问题（锁定正在读取的行）
	
	<!—->l          <!—->REPEATABLE_READ 会出幻读（锁定所读取的所有行）
	
	<!—->l          <!—->SERIALIZABLE 保证所有的情况不会发生（锁表）
	
	不可重复读的重点是修改 : 
	同样的条件 ,   你读取过的数据 ,   再次读取出来发现值不一样了 
	幻读的重点在于新增或者删除 
	同样的条件 ,   第 1 次和第 2 次读出来的记录数不一样
	
	Spring在TransactionDefinition接口中定义这些属性
	
	在TransactionDefinition接口中定义了五个不同的事务隔离级别
	
	ISOLATION_DEFAULT 这是一个PlatfromTransactionManager默认的隔离级别，使用数据库默认的事务隔离级别.另外四个与JDBC的隔离级别相对应 
	ISOLATION_READ_UNCOMMITTED 这是事务最低的隔离级别，它充许别外一个事务可以看到这个事务未提交的数据。这种隔离级别会产生脏读，不可重复读和幻像读
	
	ISOLATION_READ_COMMITTED 保证一个事务修改的数据提交后才能被另外一个事务读取。另外一个事务不能读取该事务未提交的数据。这种事务隔离级别可以避免脏读出现，但是可能会出现不可重复读和幻像读。
	
	ISOLATION_REPEATABLE_READ 这种事务隔离级别可以防止脏读，不可重复读。但是可能出现幻像读。它除了保证一个事务不能读取另一个事务未提交的数据外，还保证了避免下面的情况产生(不可重复读)。
	
	ISOLATION_SERIALIZABLE 这是花费最高代价但是最可靠的事务隔离级别。事务被处理为顺序执行。除了防止脏读，不可重复读外，还避免了幻像读。

 42.http协议
	HTTP简介
		HTTP协议是Hyper Text Transfer Protocol（超文本传输协议）的缩写,是用于从万维网（WWW:World Wide Web ）服务器传输超文本到本地浏览器的传送协议。

		HTTP是一个基于TCP/IP通信协议来传递数据（HTML 文件, 图片文件, 查询结果等）。
	
		HTTP是一个属于应用层的面向对象的协议，由于其简捷、快速的方式，适用于分布式超媒体信息系统。它于1990年提出，经过几年的使用与发展，得到不断地完善和扩展。目前在WWW中使用的是HTTP/1.0的第六版，HTTP/1.1的规范化工作正在进行之中，而且HTTP-NG(Next Generation of HTTP)的建议已经提出。
	
		HTTP协议工作于客户端-服务端架构为上。浏览器作为HTTP客户端通过URL向HTTP服务端即WEB服务器发送所有请求。Web服务器根据接收到的请求后，向客户端发送响应信息。


		http请求-响应模型.jpg
		主要特点
		1、简单快速：客户向服务器请求服务时，只需传送请求方法和路径。请求方法常用的有GET、HEAD、POST。每种方法规定了客户与服务器联系的类型不同。由于HTTP协议简单，使得HTTP服务器的程序规模小，因而通信速度很快。
	
		2、灵活：HTTP允许传输任意类型的数据对象。正在传输的类型由Content-Type加以标记。
	
		3.无连接：无连接的含义是限制每次连接只处理一个请求。服务器处理完客户的请求，并收到客户的应答后，即断开连接。采用这种方式可以节省传输时间。
	
		4.无状态：HTTP协议是无状态协议。无状态是指协议对于事务处理没有记忆能力。缺少状态意味着如果后续处理需要前面的信息，则它必须重传，这样可能导致每次连接传送的数据量增大。另一方面，在服务器不需要先前信息时它的应答就较快。
		5、支持B/S及C/S模式。
	
		HTTP之URL
		HTTP使用统一资源标识符（Uniform Resource Identifiers, URI）来传输数据和建立连接。URL是一种特殊类型的URI，包含了用于查找某个资源的足够的信息
	
		URL,全称是UniformResourceLocator, 中文叫统一资源定位符,是互联网上用来标识某一处资源的地址。以下面这个URL为例，介绍下普通URL的各部分组成：
	
		http://www.aspxfans.com:8080/news/index.asp?boardID=5&ID=24618&page=1#name
	
		从上面的URL可以看出，一个完整的URL包括以下几部分：
		1.协议部分：该URL的协议部分为“http：”，这代表网页使用的是HTTP协议。在Internet中可以使用多种协议，如HTTP，FTP等等本例中使用的是HTTP协议。在"HTTP"后面的“//”为分隔符
	
		2.域名部分：该URL的域名部分为“www.aspxfans.com”。一个URL中，也可以使用IP地址作为域名使用
	
		3.端口部分：跟在域名后面的是端口，域名和端口之间使用“:”作为分隔符。端口不是一个URL必须的部分，如果省略端口部分，将采用默认端口
	
		4.虚拟目录部分：从域名后的第一个“/”开始到最后一个“/”为止，是虚拟目录部分。虚拟目录也不是一个URL必须的部分。本例中的虚拟目录是“/news/”
	
		5.文件名部分：从域名后的最后一个“/”开始到“？”为止，是文件名部分，如果没有“?”,则是从域名后的最后一个“/”开始到“#”为止，是文件部分，如果没有“？”和“#”，那么从域名后的最后一个“/”开始到结束，都是文件名部分。本例中的文件名是“index.asp”。文件名部分也不是一个URL必须的部分，如果省略该部分，则使用默认的文件名
	
		6.锚部分：从“#”开始到最后，都是锚部分。本例中的锚部分是“name”。锚部分也不是一个URL必须的部分
	
		7.参数部分：从“？”开始到“#”为止之间的部分为参数部分，又称搜索部分、查询部分。本例中的参数部分为“boardID=5&ID=24618&page=1”。参数可以允许有多个参数，参数与参数之间用“&”作为分隔符。
	
				HTTP之请求消息Request
		客户端发送一个HTTP请求到服务器的请求消息包括以下格式：
	
		请求行（request line）、请求头部（header）、空行和请求数据四个部分组成。
	
		Http请求消息结构.png
		请求行以一个方法符号开头，以空格分开，后面跟着请求的URI和协议的版本。
		Get请求例子，使用Charles抓取的request：
	
		GET /562f25980001b1b106000338.jpg HTTP/1.1
		Host    img.mukewang.com
		User-Agent    Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.106 Safari/537.36
		Accept    image/webp,image/*,*/*;q=0.8
		Referer    http://www.imooc.com/
		Accept-Encoding    gzip, deflate, sdch
		Accept-Language    zh-CN,zh;q=0.8
		第一部分：请求行，用来说明请求类型,要访问的资源以及所使用的HTTP版本.
	
		GET说明请求类型为GET,[/562f25980001b1b106000338.jpg]为要访问的资源，该行的最后一部分说明使用的是HTTP1.1版本。
	
		第二部分：请求头部，紧接着请求行（即第一行）之后的部分，用来说明服务器要使用的附加信息
	
		从第二行起为请求头部，HOST将指出请求的目的地.User-Agent,服务器端和客户端脚本都能访问它,它是浏览器类型检测逻辑的重要基础.该信息由你的浏览器来定义,并且在每个请求中自动发送等等
	
		第三部分：空行，请求头部后面的空行是必须的
	
		即使第四部分的请求数据为空，也必须有空行。
	
		第四部分：请求数据也叫主体，可以添加任意的其他数据。
	
		这个例子的请求数据为空。
	
		POST请求例子，使用Charles抓取的request：
	
		POST / HTTP1.1
		Host:www.wrox.com
		User-Agent:Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; .NET CLR 2.0.50727; .NET CLR 3.0.04506.648; .NET CLR 3.5.21022)
		Content-Type:application/x-www-form-urlencoded
		Content-Length:40
		Connection: Keep-Alive
	
		name=Professional%20Ajax&publisher=Wiley
		第一部分：请求行，第一行明了是post请求，以及http1.1版本。
		第二部分：请求头部，第二行至第六行。
		第三部分：空行，第七行的空行。
		第四部分：请求数据，第八行。
	
		HTTP之响应消息Response
		一般情况下，服务器接收并处理客户端发过来的请求后会返回一个HTTP的响应消息。
	
		HTTP响应也由四个部分组成，分别是：状态行、消息报头、空行和响应正文。


​		 

​	http响应消息格式.jpg
​	例子

​	HTTP/1.1 200 OK
​	Date: Fri, 22 May 2009 06:07:21 GMT
​	Content-Type: text/html; charset=UTF-8

​	<html>
		  <head></head>
​		  <body>
​				<!--body goes here-->
​		  </body>
​	</html>
​	第一部分：状态行，由HTTP协议版本号， 状态码， 状态消息 三部分组成。

​	第一行为状态行，（HTTP/1.1）表明HTTP版本为1.1版本，状态码为200，状态消息为（ok）

​	第二部分：消息报头，用来说明客户端要使用的一些附加信息

​	第二行和第三行为消息报头，
​	Date:生成响应的日期和时间；Content-Type:指定了MIME类型的HTML(text/html),编码类型是UTF-8

​	第三部分：空行，消息报头后面的空行是必须的

​	第四部分：响应正文，服务器返回给客户端的文本信息。

​	空行后面的html部分为响应正文。

​	HTTP之状态码
​	状态代码有三位数字组成，第一个数字定义了响应的类别，共分五种类别:

​	1xx：指示信息--表示请求已接收，继续处理

​	2xx：成功--表示请求已被成功接收、理解、接受

​	3xx：重定向--要完成请求必须进行更进一步的操作

​	4xx：客户端错误--请求有语法错误或请求无法实现

​	5xx：服务器端错误--服务器未能实现合法的请求

​	常见状态码：

​	200 OK                        //客户端请求成功
​	400 Bad Request               //客户端请求有语法错误，不能被服务器所理解
​	401 Unauthorized              //请求未经授权，这个状态代码必须和WWW-Authenticate报头域一起使用 
​	403 Forbidden                 //服务器收到请求，但是拒绝提供服务
​	404 Not Found                 //请求资源不存在，eg：输入了错误的URL
​	500 Internal Server Error     //服务器发生不可预期的错误
​	503 Server Unavailable        //服务器当前不能处理客户端的请求，一段时间后可能恢复正常
​	更多状态码http://www.runoob.com/http/http-status-codes.html

​	HTTP请求方法
​	根据HTTP标准，HTTP请求可以使用多种请求方法。
​	HTTP1.0定义了三种请求方法： GET, POST 和 HEAD方法。
​	HTTP1.1新增了五种请求方法：OPTIONS, PUT, DELETE, TRACE 和 CONNECT 方法。

​	GET     请求指定的页面信息，并返回实体主体。
​	HEAD     类似于get请求，只不过返回的响应中没有具体的内容，用于获取报头
​	POST     向指定资源提交数据进行处理请求（例如提交表单或者上传文件）。数据被包含在请求体中。POST请求可能会导致新的资源的建立和/或已有资源的修改。
​	PUT     从客户端向服务器传送的数据取代指定的文档的内容。
​	DELETE      请求服务器删除指定的页面。
​	CONNECT     HTTP/1.1协议中预留给能够将连接改为管道方式的代理服务器。
​	OPTIONS     允许客户端查看服务器的性能。
​	TRACE     回显服务器收到的请求，主要用于测试或诊断。
​	HTTP工作原理
​	HTTP协议定义Web客户端如何从Web服务器请求Web页面，以及服务器如何把Web页面传送给客户端。HTTP协议采用了请求/响应模型。客户端向服务器发送一个请求报文，请求报文包含请求的方法、URL、协议版本、请求头部和请求数据。服务器以一个状态行作为响应，响应的内容包括协议的版本、成功或者错误代码、服务器信息、响应头部和响应数据。

​	以下是 HTTP 请求/响应的步骤：

​	1、客户端连接到Web服务器

​	一个HTTP客户端，通常是浏览器，与Web服务器的HTTP端口（默认为80）建立一个TCP套接字连接。例如，http://www.oakcms.cn。

​	2、发送HTTP请求

​	通过TCP套接字，客户端向Web服务器发送一个文本的请求报文，一个请求报文由请求行、请求头部、空行和请求数据4部分组成。

​	3、服务器接受请求并返回HTTP响应

​	Web服务器解析请求，定位请求资源。服务器将资源复本写到TCP套接字，由客户端读取。一个响应由状态行、响应头部、空行和响应数据4部分组成。

​	4、释放连接TCP连接

​	若connection 模式为close，则服务器主动关闭TCP连接，客户端被动关闭连接，释放TCP连接;若connection 模式为keepalive，则该连接会保持一段时间，在该时间内可以继续接收请求;

​	5、客户端浏览器解析HTML内容

​	客户端浏览器首先解析状态行，查看表明请求是否成功的状态代码。然后解析每一个响应头，响应头告知以下为若干字节的HTML文档和文档的字符集。客户端浏览器读取响应数据HTML，根据HTML的语法对其进行格式化，并在浏览器窗口中显示。

​	例如：在浏览器地址栏键入URL，按下回车之后会经历以下流程：

​	1、浏览器向 DNS 服务器请求解析该 URL 中的域名所对应的 IP 地址;

​	2、解析出 IP 地址后，根据该 IP 地址和默认端口 80，和服务器建立TCP连接;

​	3、浏览器发出读取文件(URL 中域名后面部分对应的文件)的HTTP 请求，该请求报文作为 TCP 三次握手的第三个报文的数据发送给服务器;

​	4、服务器对浏览器请求作出响应，并把对应的 html 文本发送给浏览器;

​	5、释放 TCP连接;

​	6、浏览器将该 html 文本并显示内容; 　　

​	GET和POST请求的区别
​	GET请求

​	GET /books/?sex=man&name=Professional HTTP/1.1
​	Host: www.wrox.com
​	User-Agent: Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.7.6)
​	Gecko/20050225 Firefox/1.0.1
​	Connection: Keep-Alive
​	注意最后一行是空行

​	POST请求

​	POST / HTTP/1.1
​	Host: www.wrox.com
​	User-Agent: Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.7.6)
​	Gecko/20050225 Firefox/1.0.1
​	Content-Type: application/x-www-form-urlencoded
​	Content-Length: 40
​	Connection: Keep-Alive

​	name=Professional%20Ajax&publisher=Wiley
​	1、GET提交，请求的数据会附在URL之后（就是把数据放置在HTTP协议头中），以?分割URL和传输数据，多个参数用&连接；例 如：login.action?name=hyddd&password=idontknow&verify=%E4%BD%A0 %E5%A5%BD。如果数据是英文字母/数字，原样发送，如果是空格，转换为+，如果是中文/其他字符，则直接把字符串用BASE64加密，得出如： %E4%BD%A0%E5%A5%BD，其中％XX中的XX为该符号以16进制表示的ASCII。

​	POST提交：把提交的数据放置在是HTTP包的包体中。上文示例中红色字体标明的就是实际的传输数据

​	因此，GET提交的数据会在地址栏中显示出来，而POST提交，地址栏不会改变

​	2、传输数据的大小：首先声明：HTTP协议没有对传输的数据大小进行限制，HTTP协议规范也没有对URL长度进行限制。

​	而在实际开发中存在的限制主要有：

​	GET:特定浏览器和服务器对URL长度有限制，例如 IE对URL长度的限制是2083字节(2K+35)。对于其他浏览器，如Netscape、FireFox等，理论上没有长度限制，其限制取决于操作系 统的支持。

​	因此对于GET提交时，传输数据就会受到URL长度的 限制。

​	POST:由于不是通过URL传值，理论上数据不受 限。但实际各个WEB服务器会规定对post提交数据大小进行限制，Apache、IIS6都有各自的配置。

​	3、安全性

​	POST的安全性要比GET的安全性高。比如：通过GET提交数据，用户名和密码将明文出现在URL上，因为(1)登录页面有可能被浏览器缓存；(2)其他人查看浏览器的历史纪录，那么别人就可以拿到你的账号和密码了，除此之外，使用GET提交数据还可能会造成Cross-site request forgery攻击

​	4、Http get,post,soap协议都是在http上运行的

​	（1）get：请求参数是作为一个key/value对的序列（查询字符串）附加到URL上的
​	查询字符串的长度受到web浏览器和web服务器的限制（如IE最多支持2048个字符），不适合传输大型数据集同时，它很不安全

​	（2）post：请求参数是在http标题的一个不同部分（名为entity body）传输的，这一部分用来传输表单信息，因此必须将Content-type设置为:application/x-www-form- urlencoded。post设计用来支持web窗体上的用户字段，其参数也是作为key/value对传输。
​	但是：它不支持复杂数据类型，因为post没有定义传输数据结构的语义和规则。

​	（3）soap：是http post的一个专用版本，遵循一种特殊的xml消息格式
​	Content-type设置为: text/xml 任何数据都可以xml化。

​	Http协议定义了很多与服务器交互的方法，最基本的有4种，分别是GET,POST,PUT,DELETE. 一个URL地址用于描述一个网络上的资源，而HTTP中的GET, POST, PUT, DELETE就对应着对这个资源的查，改，增，删4个操作。 我们最常见的就是GET和POST了。GET一般用于获取/查询资源信息，而POST一般用于更新资源信息.

​	我们看看GET和POST的区别

​	GET提交的数据会放在URL之后，以?分割URL和传输数据，参数之间以&相连，如EditPosts.aspx?name=test1&id=123456. POST方法是把提交的数据放在HTTP包的Body中.

​	GET提交的数据大小有限制（因为浏览器对URL的长度有限制），而POST方法提交的数据没有限制.

​	GET方式需要使用Request.QueryString来取得变量的值，而POST方式通过Request.Form来获取变量的值。

​	GET方式提交数据，会带来安全问题，比如一个登录页面，通过GET方式提交数据时，用户名和密码将出现在URL上，如果页面可以被缓存或者其他人可以访问这台机器，就可以从历史记录获得该用户的账号和密码.

一、HTTP和HTTPS的基本概念

​	HTTP：是互联网上应用最为广泛的一种网络协议，是一个客户端和服务器端请求和应答的标准（TCP），用于从WWW服务器传输超文本到本地浏览器的传输协议，它可以使浏览器更加高效，使网络传输减少。

​	HTTPS：是以安全为目标的HTTP通道，简单讲是HTTP的安全版，即HTTP下加入SSL层，HTTPS的安全基础是SSL，因此加密的详细内容就需要SSL。

​	HTTPS协议的主要作用可以分为两种：一种是建立一个信息安全通道，来保证数据传输的安全；另一种就是确认网站的真实性。

二、HTTP与HTTPS有什么区别？

​	HTTP协议传输的数据都是未加密的，也就是明文的，因此使用HTTP协议传输隐私信息非常不安全，为了保证这些隐私数据能加密传输，于是网景公司设计了SSL（Secure Sockets Layer）协议用于对HTTP协议传输的数据进行加密，从而就诞生了HTTPS。

​	简单来说，HTTPS协议是由SSL+HTTP协议构建的可进行加密传输、身份认证的网络协议，要比http协议安全。

​	HTTPS和HTTP的区别主要如下：

​	1、https协议需要到ca申请证书，一般免费证书较少，因而需要一定费用。

​	2、http是超文本传输协议，信息是明文传输，https则是具有安全性的ssl加密传输协议。

​	3、http和https使用的是完全不同的连接方式，用的端口也不一样，前者是80，后者是443。

​	4、http的连接很简单，是无状态的；HTTPS协议是由SSL+HTTP协议构建的可进行加密传输、身份认证的网络协议，比http协议安全。

三、HTTPS的工作原理

​	我们都知道HTTPS能够加密信息，以免敏感信息被第三方获取，所以很多银行网站或电子邮箱等等安全级别较高的服务都会采用HTTPS协议。

​	1、客户端发起HTTPS请求

​	这个没什么好说的，就是用户在浏览器里输入一个https网址，然后连接到server的443端口。

​	2、服务端的配置

​	采用HTTPS协议的服务器必须要有一套数字证书，可以自己制作，也可以向组织申请，区别就是自己颁发的证书需要客户端验证通过，才可以继续访问，而使用受信任的公司申请的证书则不会弹出提示页面(startssl就是个不错的选择，有1年的免费服务)。

​	这套证书其实就是一对公钥和私钥，如果对公钥和私钥不太理解，可以想象成一把钥匙和一个锁头，只是全世界只有你一个人有这把钥匙，你可以把锁头给别人，别人可以用这个锁把重要的东西锁起来，然后发给你，因为只有你一个人有这把钥匙，所以只有你才能看到被这把锁锁起来的东西。

​	3、传送证书

​	这个证书其实就是公钥，只是包含了很多信息，如证书的颁发机构，过期时间等等。

​	4、客户端解析证书

​	这部分工作是有客户端的TLS来完成的，首先会验证公钥是否有效，比如颁发机构，过期时间等等，如果发现异常，则会弹出一个警告框，提示证书存在问题。

​	如果证书没有问题，那么就生成一个随机值，然后用证书对该随机值进行加密，就好像上面说的，把随机值用锁头锁起来，这样除非有钥匙，不然看不到被锁住的内容。

​	5、传送加密信息

​	这部分传送的是用证书加密后的随机值，目的就是让服务端得到这个随机值，以后客户端和服务端的通信就可以通过这个随机值来进行加密解密了。

​	6、服务段解密信息

​	服务端用私钥解密后，得到了客户端传过来的随机值(私钥)，然后把内容通过该值进行对称加密，所谓对称加密就是，将信息和私钥通过某种算法混合在一起，这样除非知道私钥，不然无法获取内容，而正好客户端和服务端都知道这个私钥，所以只要加密算法够彪悍，私钥够复杂，数据就够安全。

​	7、传输加密后的信息

​	这部分信息是服务段用私钥加密后的信息，可以在客户端被还原。

​	8、客户端解密信息

​	客户端用之前生成的私钥解密服务段传过来的信息，于是获取了解密后的内容，整个过程第三方即使监听到了数据，也束手无策。

六、HTTPS的优点

​	正是由于HTTPS非常的安全，攻击者无法从中找到下手的地方，从站长的角度来说，HTTPS的优点有以下2点：

​	1、SEO方面

​	谷歌曾在2014年8月份调整搜索引擎算法，并称“比起同等HTTP网站，采用HTTPS加密的网站在搜索结果中的排名将会更高”。

​	2、安全性

​	尽管HTTPS并非绝对安全，掌握根证书的机构、掌握加密算法的组织同样可以进行中间人形式的攻击，但HTTPS仍是现行架构下最安全的解决方案，主要有以下几个好处：

​	（1）、使用HTTPS协议可认证用户和服务器，确保数据发送到正确的客户机和服务器；

​	（2）、HTTPS协议是由SSL+HTTP协议构建的可进行加密传输、身份认证的网络协议，要比http协议安全，可防止数据在传输过程中不被窃取、改变，确保数据的完整性。

​	（3）、HTTPS是现行架构下最安全的解决方案，虽然不是绝对安全，但它大幅增加了中间人攻击的成本。

七、HTTPS的缺点

​	虽然说HTTPS有很大的优势，但其相对来说，还是有些不足之处的，具体来说，有以下2点：

​	1、SEO方面

​	据ACM CoNEXT数据显示，使用HTTPS协议会使页面的加载时间延长近50%，增加10%到20%的耗电，此外，HTTPS协议还会影响缓存，增加数据开销和功耗，甚至已有安全措施也会受到影响也会因此而受到影响。

​	而且HTTPS协议的加密范围也比较有限，在黑客攻击、拒绝服务攻击、服务器劫持等方面几乎起不到什么作用。

​	最关键的，SSL证书的信用链体系并不安全，特别是在某些国家可以控制CA根证书的情况下，中间人攻击一样可行。

​	2、经济方面

​	（1）、SSL证书需要钱，功能越强大的证书费用越高，个人网站、小网站没有必要一般不会用。

​	（2）、SSL证书通常需要绑定IP，不能在同一IP上绑定多个域名，IPv4资源不可能支撑这个消耗（SSL有扩展可以部分解决这个问题，但是比较麻烦，而且要求浏览器、操作系统支持，Windows XP就不支持这个扩展，考虑到XP的装机量，这个特性几乎没用）。

​	（3）、HTTPS连接缓存不如HTTP高效，大流量网站如非必要也不会采用，流量成本太高。

​	（4）、HTTPS连接服务器端资源占用高很多，支持访客稍多的网站需要投入更大的成本，如果全部采用HTTPS，基于大部分计算资源闲置的假设的VPS的平均成本会上去。

​	（5）、HTTPS协议握手阶段比较费时，对网站的相应速度有负面影响，如非必要，没有理由牺牲用户体验。